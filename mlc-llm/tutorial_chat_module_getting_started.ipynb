{"cells":[{"cell_type":"markdown","metadata":{"id":"Cm85Ap3zDmYB"},"source":["# Getting Started with MLC-LLM using the Llama 2 Model\n","\n","Here's a quick overview of how to get started with the MLC-LLM `ChatModule` in Python. In this tutorial, we will chat with the [Llama 2](https://ai.meta.com/llama/) model. For the easiest setup, we recommend trying this out in a Google Colab notebook. Click the button below to get started!\n","\n","<a target=\"_blank\" href=\"https://colab.research.google.com/github/mlc-ai/notebooks/blob/main/mlc-llm/tutorial_chat_module_getting_started.ipynb\">\n","  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n","</a>"]},{"cell_type":"markdown","metadata":{"id":"1ttPt-hNDmYC"},"source":["## Environment Setup\n","\n","Let's set up your environment, so you can successfully run the `ChatModule`. First, lets set up the Conda environment which we'll be running this notebook in (not required if running in Google Colab).\n","\n","```bash\n","conda create --name mlc-llm python=3.10\n","conda activate mlc-llm\n","```\n","\n","**Google Colab:** If you are running this in a Google Colab notebook, be sure to change your runtime to GPU by going to Runtime > Change runtime type and setting the Hardware accelerator to be \"GPU\". Select \"Connect\" on the top right to instantiate your GPU session.\n","\n","If you are using CUDA, you can run the following command to confirm that CUDA is set up correctly, and check the version number."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KK25HZsIDmYC","outputId":"67e46ce2-a1dd-41b8-e374-487b8df851e1","executionInfo":{"status":"ok","timestamp":1691460425512,"user_tz":300,"elapsed":234,"user":{"displayName":"Sachin Beldona","userId":"08409251876177109108"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2022 NVIDIA Corporation\n","Built on Wed_Sep_21_10:33:58_PDT_2022\n","Cuda compilation tools, release 11.8, V11.8.89\n","Build cuda_11.8.r11.8/compiler.31833905_0\n"]}],"source":["!nvidia-smi"]},{"cell_type":"markdown","metadata":{"id":"EWOtpjJMDmYE"},"source":["Next, let's download the MLC-AI and MLC-Chat nightly build packages. Go to https://mlc.ai/package/ and replace the command below with the one that is appropriate for your hardware and OS."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"PgW-5OAADmYE","outputId":"848a15da-1605-462d-c164-83993030aeba","executionInfo":{"status":"ok","timestamp":1691460483979,"user_tz":300,"elapsed":41627,"user":{"displayName":"Sachin Beldona","userId":"08409251876177109108"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in links: https://mlc.ai/wheels\n","Collecting mlc-ai-nightly-cu118\n","  Downloading https://github.com/mlc-ai/package/releases/download/v0.9.dev0/mlc_ai_nightly_cu118-0.12.dev1385-cp310-cp310-manylinux_2_28_x86_64.whl (97.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting mlc-chat-nightly-cu118\n","  Downloading https://github.com/mlc-ai/package/releases/download/v0.9.dev0/mlc_chat_nightly_cu118-0.1.dev328-cp310-cp310-manylinux_2_28_x86_64.whl (20.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting attrs (from mlc-ai-nightly-cu118)\n","  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting cloudpickle (from mlc-ai-nightly-cu118)\n","  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n","Collecting decorator (from mlc-ai-nightly-cu118)\n","  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n","Collecting ml-dtypes (from mlc-ai-nightly-cu118)\n","  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting numpy (from mlc-ai-nightly-cu118)\n","  Downloading numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting psutil (from mlc-ai-nightly-cu118)\n","  Downloading psutil-5.9.5-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (282 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m282.1/282.1 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting scipy (from mlc-ai-nightly-cu118)\n","  Downloading scipy-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.3/36.3 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tornado (from mlc-ai-nightly-cu118)\n","  Downloading tornado-6.3.2-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (426 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.9/426.9 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting typing-extensions (from mlc-ai-nightly-cu118)\n","  Downloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n","Collecting fastapi (from mlc-chat-nightly-cu118)\n","  Downloading fastapi-0.101.0-py3-none-any.whl (65 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting uvicorn (from mlc-chat-nightly-cu118)\n","  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting shortuuid (from mlc-chat-nightly-cu118)\n","  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n","Collecting pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 (from fastapi->mlc-chat-nightly-cu118)\n","  Downloading pydantic-2.1.1-py3-none-any.whl (370 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m370.9/370.9 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting starlette<0.28.0,>=0.27.0 (from fastapi->mlc-chat-nightly-cu118)\n","  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting click>=7.0 (from uvicorn->mlc-chat-nightly-cu118)\n","  Downloading click-8.1.6-py3-none-any.whl (97 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting h11>=0.8 (from uvicorn->mlc-chat-nightly-cu118)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting annotated-types>=0.4.0 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi->mlc-chat-nightly-cu118)\n","  Downloading annotated_types-0.5.0-py3-none-any.whl (11 kB)\n","Collecting pydantic-core==2.4.0 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi->mlc-chat-nightly-cu118)\n","  Downloading pydantic_core-2.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting anyio<5,>=3.4.0 (from starlette<0.28.0,>=0.27.0->fastapi->mlc-chat-nightly-cu118)\n","  Downloading anyio-4.0.0rc1-py3-none-any.whl (82 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.8/82.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting idna>=2.8 (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi->mlc-chat-nightly-cu118)\n","  Downloading idna-3.4-py3-none-any.whl (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sniffio>=1.1 (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi->mlc-chat-nightly-cu118)\n","  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n","Collecting exceptiongroup>=1.0.2 (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi->mlc-chat-nightly-cu118)\n","  Downloading exceptiongroup-1.1.2-py3-none-any.whl (14 kB)\n","Installing collected packages: typing-extensions, tornado, sniffio, shortuuid, psutil, numpy, idna, h11, exceptiongroup, decorator, cloudpickle, click, attrs, annotated-types, uvicorn, scipy, pydantic-core, ml-dtypes, anyio, starlette, pydantic, mlc-ai-nightly-cu118, fastapi, mlc-chat-nightly-cu118\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing_extensions 4.7.1\n","    Uninstalling typing_extensions-4.7.1:\n","      Successfully uninstalled typing_extensions-4.7.1\n","  Attempting uninstall: tornado\n","    Found existing installation: tornado 6.3.1\n","    Uninstalling tornado-6.3.1:\n","      Successfully uninstalled tornado-6.3.1\n","  Attempting uninstall: sniffio\n","    Found existing installation: sniffio 1.3.0\n","    Uninstalling sniffio-1.3.0:\n","      Successfully uninstalled sniffio-1.3.0\n","  Attempting uninstall: psutil\n","    Found existing installation: psutil 5.9.5\n","    Uninstalling psutil-5.9.5:\n","      Successfully uninstalled psutil-5.9.5\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.22.4\n","    Uninstalling numpy-1.22.4:\n","      Successfully uninstalled numpy-1.22.4\n","  Attempting uninstall: idna\n","    Found existing installation: idna 3.4\n","    Uninstalling idna-3.4:\n","      Successfully uninstalled idna-3.4\n","  Attempting uninstall: exceptiongroup\n","    Found existing installation: exceptiongroup 1.1.2\n","    Uninstalling exceptiongroup-1.1.2:\n","      Successfully uninstalled exceptiongroup-1.1.2\n","  Attempting uninstall: decorator\n","    Found existing installation: decorator 4.4.2\n","    Uninstalling decorator-4.4.2:\n","      Successfully uninstalled decorator-4.4.2\n","  Attempting uninstall: cloudpickle\n","    Found existing installation: cloudpickle 2.2.1\n","    Uninstalling cloudpickle-2.2.1:\n","      Successfully uninstalled cloudpickle-2.2.1\n","  Attempting uninstall: click\n","    Found existing installation: click 8.1.6\n","    Uninstalling click-8.1.6:\n","      Successfully uninstalled click-8.1.6\n","  Attempting uninstall: attrs\n","    Found existing installation: attrs 23.1.0\n","    Uninstalling attrs-23.1.0:\n","      Successfully uninstalled attrs-23.1.0\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.10.1\n","    Uninstalling scipy-1.10.1:\n","      Successfully uninstalled scipy-1.10.1\n","  Attempting uninstall: ml-dtypes\n","    Found existing installation: ml-dtypes 0.2.0\n","    Uninstalling ml-dtypes-0.2.0:\n","      Successfully uninstalled ml-dtypes-0.2.0\n","  Attempting uninstall: anyio\n","    Found existing installation: anyio 3.7.1\n","    Uninstalling anyio-3.7.1:\n","      Successfully uninstalled anyio-3.7.1\n","  Attempting uninstall: pydantic\n","    Found existing installation: pydantic 1.10.12\n","    Uninstalling pydantic-1.10.12:\n","      Successfully uninstalled pydantic-1.10.12\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipython 7.34.0 requires jedi>=0.16, which is not installed.\n","confection 0.1.0 requires pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4, but you have pydantic 2.1.1 which is incompatible.\n","google-colab 1.0.0 requires tornado==6.3.1, but you have tornado 6.3.2 which is incompatible.\n","inflect 6.0.5 requires pydantic<2,>=1.9.1, but you have pydantic 2.1.1 which is incompatible.\n","jupyter-server 1.24.0 requires anyio<4,>=3.1.0, but you have anyio 4.0.0rc1 which is incompatible.\n","moviepy 1.0.3 requires decorator<5.0,>=4.0.2, but you have decorator 5.1.1 which is incompatible.\n","numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.25.2 which is incompatible.\n","spacy 3.5.4 requires pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4, but you have pydantic 2.1.1 which is incompatible.\n","tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.25.2 which is incompatible.\n","thinc 8.1.10 requires pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4, but you have pydantic 2.1.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed annotated-types-0.5.0 anyio-4.0.0rc1 attrs-23.1.0 click-8.1.6 cloudpickle-2.2.1 decorator-5.1.1 exceptiongroup-1.1.2 fastapi-0.101.0 h11-0.14.0 idna-3.4 ml-dtypes-0.2.0 mlc-ai-nightly-cu118-0.12.dev1385 mlc-chat-nightly-cu118-0.1.dev328 numpy-1.25.2 psutil-5.9.5 pydantic-2.1.1 pydantic-core-2.4.0 scipy-1.11.1 shortuuid-1.0.11 sniffio-1.3.0 starlette-0.27.0 tornado-6.3.2 typing-extensions-4.7.1 uvicorn-0.23.2\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["decorator","numpy","psutil","tornado"]}}},"metadata":{}}],"source":["!pip install --pre --force-reinstall mlc-ai-nightly-cu118 mlc-chat-nightly-cu118 -f https://mlc.ai/wheels"]},{"cell_type":"markdown","source":["**Google Colab:** If in Google Colab, you may see a message warning you to restart the runtime. Simply run the following code in a new code cell to restart the runtime.\n","\n","```python\n","import os\n","os.kill(os.getpid(), 9)\n","```"],"metadata":{"id":"Jn7MYEFt5tvY"}},{"cell_type":"markdown","metadata":{"id":"FwsWd1WbDmYE"},"source":["Next, let's download the model weights for the Llama 2 model and the prebuilt model libraries from Github. In order to download the large weights, we'll have to use `git lfs`."]},{"cell_type":"markdown","source":["Note: If you are NOT running in **Google Colab** you may need to run this line `!conda install git git-lfs` to install `git` and `git-lfs` before running the following cell to fully install `git lfs`."],"metadata":{"id":"ppvAhErV3gjq"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V0GjINnMDmYF","outputId":"165a151c-1181-46cd-ec3d-9fdf81c1068a","executionInfo":{"status":"ok","timestamp":1691460739962,"user_tz":300,"elapsed":228,"user":{"displayName":"Sachin Beldona","userId":"08409251876177109108"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Git LFS initialized.\n"]}],"source":["!git lfs install"]},{"cell_type":"markdown","source":["These commands will download many prebuilt libraries as well as the chat configuration for Llama-2-7b that `mlc_chat` needs, which may take a long time. If in **Google Colab** you can verify that the files are being downloaded by clicking on the folder icon on the left and navigating to the `dist` and then `prebuilt` folders which should be updating as the files are being downloaded."],"metadata":{"id":"yYwjsCOK7Jij"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FSAe7Ew_DmYF","outputId":"8b291cb7-6cb0-4652-bd8c-216b2eb3d9f5","executionInfo":{"status":"ok","timestamp":1691461003613,"user_tz":300,"elapsed":286,"user":{"displayName":"Sachin Beldona","userId":"08409251876177109108"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'dist/prebuilt/lib' already exists and is not an empty directory.\n"]}],"source":["!mkdir -p dist/prebuilt\n","!git clone https://github.com/mlc-ai/binary-mlc-llm-libs.git dist/prebuilt/lib"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BDbi6H3MDmYF","outputId":"f9c878f0-f5ba-4755-8306-84c50e26b914","executionInfo":{"status":"ok","timestamp":1691461008810,"user_tz":300,"elapsed":229,"user":{"displayName":"Sachin Beldona","userId":"08409251876177109108"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'mlc-chat-Llama-2-7b-chat-hf-q4f16_1' already exists and is not an empty directory.\n"]}],"source":["!cd dist/prebuilt && git clone https://huggingface.co/mlc-ai/mlc-chat-Llama-2-7b-chat-hf-q4f16_1"]},{"cell_type":"markdown","metadata":{"id":"76Ru5__tDmYF"},"source":["## Let's Chat!\n","\n","Before we can chat with the model, we must first import a library and instantiate a `ChatModule` instance. The `ChatModule` must be initialized with the appropriate model name."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AJAt6oW7DmYF","outputId":"bad7995b-c0a6-4260-dc21-dcf989fe455c","executionInfo":{"status":"ok","timestamp":1691461025129,"user_tz":300,"elapsed":13122,"user":{"displayName":"Sachin Beldona","userId":"08409251876177109108"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Using model folder: /content/dist/prebuilt/mlc-chat-Llama-2-7b-chat-hf-q4f16_1\n","Using mlc chat config: /content/dist/prebuilt/mlc-chat-Llama-2-7b-chat-hf-q4f16_1/mlc-chat-config.json\n","Using library model: /content/dist/prebuilt/lib/Llama-2-7b-chat-hf-q4f16_1-cuda.so\n"]}],"source":["from mlc_chat import ChatModule\n","\n","cm = ChatModule(model=\"Llama-2-7b-chat-hf-q4f16_1\", device_name=\"cuda\")"]},{"cell_type":"markdown","metadata":{"id":"c9m5sxyXDmYF"},"source":["Note that the above invocation abstracts away the logic for finding the relevant model directory and prebuilt library paths. To specify these manually, you could run the following instead (which would be equivalent to the above).\n","\n","```python\n","cm = ChatModule(model=\"dist/prebuilt/mlc-chat-Llama-2-7b-chat-hf-q4f16_1\", lib_path=\"dist/prebuilt/lib/Llama-2-7b-chat-hf-q4f16_1-cuda.so\")\n","```"]},{"cell_type":"markdown","metadata":{"id":"zEaVXnnJDmYF"},"source":["That's all that's needed to set up the `ChatModule`. You can now chat with the model by inputting any prompt you'd like. Try it out below!"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TNmg9N_NDmYF","outputId":"3f3ab943-a8f7-4423-ccc9-dc90d86cfb6e","executionInfo":{"status":"ok","timestamp":1691461051552,"user_tz":300,"elapsed":9444,"user":{"displayName":"Sachin Beldona","userId":"08409251876177109108"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Prompt: What is CUDA?\n","Hello! I'm here to help you with your question. CUDA is a technology that stands for Compute Unified Device Architecture. It's a parallel computing platform and programming model developed by NVIDIA that allows developers to use the massively parallel processing power of NVIDIA GPUs (Graphics Processing Units) to accelerate various tasks, such as machine learning, scientific simulations, and data analysis.\n","\n","CUDA allows developers to write programs that can execute on the parallel processing cores inside an NVIDIA GPU, which can perform many calculations simultaneously, much faster than a traditional CPU (Central Processing Unit). This enables developers to solve complex problems much faster and more efficiently, making it a popular choice for various fields such as machine learning, deep learning, and data science.\n","\n","Is there anything else you would like to know about CUDA?\n"]}],"source":["prompt = input(\"Prompt: \")\n","cm.generate(prompt=prompt)"]},{"cell_type":"markdown","metadata":{"id":"I4bOyUk7DmYF"},"source":["To evaluate the speed of the chat bot, you can print some statistics."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"PPbPj6vpDmYF","outputId":"d0a6be1f-4be7-4bc3-dd5a-1d8aeb59d155","executionInfo":{"status":"ok","timestamp":1691461057485,"user_tz":300,"elapsed":91,"user":{"displayName":"Sachin Beldona","userId":"08409251876177109108"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'prefill: 477.8 tok/s, decode: 43.6 tok/s'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":10}],"source":["cm.runtime_stats_text()"]},{"cell_type":"markdown","metadata":{"id":"XAb-XZPnDmYF"},"source":["By default, the `ChatModule` will keep a history of your chat. You can reset the chat history by running the following."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iKpKgVxNDmYF"},"outputs":[],"source":["cm.reset_chat()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"orig_nbformat":4,"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}