{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with MLC-LLM in Python\n",
    "\n",
    "Here's a quick overview of how to get started with the MLC-LLM `ChatModule` in Python. In this tutorial, we will chat with the [Vicuna-7B](https://huggingface.co/lmsys/vicuna-7b-delta-v1.1) model, which was trained by fine-tuning LLaMa and developed by LMSYS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Let's set up your environment, so you can successfully run the `ChatModule`. First, lets set up the Conda environment which we'll be running this notebook in.\n",
    "\n",
    "```bash\n",
    "conda create --name mlc-llm python=3.10\n",
    "conda activate mlc-llm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's download the MLC-AI and MLC-Chat nightly build packages. Go to https://mlc.ai/package/ and replace the command below with the one that is appropriate for your hardware and OS. Let's say we are using CUDA 11.6 on Linux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://mlc.ai/wheels\n",
      "Collecting mlc-ai-nightly-cu116\n",
      "  Using cached https://github.com/mlc-ai/package/releases/download/v0.9.dev0/mlc_ai_nightly_cu116-0.12.dev1265-cp310-cp310-manylinux_2_28_x86_64.whl (98.6 MB)\n",
      "Collecting mlc-chat-nightly-cu116\n",
      "  Downloading https://github.com/mlc-ai/package/releases/download/v0.9.dev0/mlc_chat_nightly_cu116-0.1.dev257-cp310-cp310-manylinux_2_28_x86_64.whl (19.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting attrs (from mlc-ai-nightly-cu116)\n",
      "  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "Collecting cloudpickle (from mlc-ai-nightly-cu116)\n",
      "  Using cached cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Collecting decorator (from mlc-ai-nightly-cu116)\n",
      "  Using cached decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
      "Collecting ml-dtypes (from mlc-ai-nightly-cu116)\n",
      "  Using cached ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "Collecting numpy (from mlc-ai-nightly-cu116)\n",
      "  Using cached numpy-1.25.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
      "Collecting psutil (from mlc-ai-nightly-cu116)\n",
      "  Using cached psutil-5.9.5-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (282 kB)\n",
      "Collecting scipy (from mlc-ai-nightly-cu116)\n",
      "  Using cached scipy-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.3 MB)\n",
      "Collecting tornado (from mlc-ai-nightly-cu116)\n",
      "  Using cached tornado-6.3.2-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (426 kB)\n",
      "Collecting typing-extensions (from mlc-ai-nightly-cu116)\n",
      "  Using cached typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Collecting fastapi (from mlc-chat-nightly-cu116)\n",
      "  Using cached fastapi-0.100.0-py3-none-any.whl (65 kB)\n",
      "Collecting uvicorn (from mlc-chat-nightly-cu116)\n",
      "  Using cached uvicorn-0.23.0-py3-none-any.whl (59 kB)\n",
      "Collecting shortuuid (from mlc-chat-nightly-cu116)\n",
      "  Using cached shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 (from fastapi->mlc-chat-nightly-cu116)\n",
      "  Using cached pydantic-2.0.3-py3-none-any.whl (364 kB)\n",
      "Collecting starlette<0.28.0,>=0.27.0 (from fastapi->mlc-chat-nightly-cu116)\n",
      "  Using cached starlette-0.27.0-py3-none-any.whl (66 kB)\n",
      "Collecting click>=7.0 (from uvicorn->mlc-chat-nightly-cu116)\n",
      "  Using cached click-8.1.5-py3-none-any.whl (98 kB)\n",
      "Collecting h11>=0.8 (from uvicorn->mlc-chat-nightly-cu116)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->fastapi->mlc-chat-nightly-cu116)\n",
      "  Using cached annotated_types-0.5.0-py3-none-any.whl (11 kB)\n",
      "Collecting pydantic-core==2.3.0 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->fastapi->mlc-chat-nightly-cu116)\n",
      "  Using cached pydantic_core-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
      "Collecting anyio<5,>=3.4.0 (from starlette<0.28.0,>=0.27.0->fastapi->mlc-chat-nightly-cu116)\n",
      "  Using cached anyio-3.7.1-py3-none-any.whl (80 kB)\n",
      "Collecting idna>=2.8 (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi->mlc-chat-nightly-cu116)\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting sniffio>=1.1 (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi->mlc-chat-nightly-cu116)\n",
      "  Using cached sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Collecting exceptiongroup (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi->mlc-chat-nightly-cu116)\n",
      "  Using cached exceptiongroup-1.1.2-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: typing-extensions, tornado, sniffio, shortuuid, psutil, numpy, idna, h11, exceptiongroup, decorator, cloudpickle, click, attrs, annotated-types, uvicorn, scipy, pydantic-core, ml-dtypes, anyio, starlette, pydantic, mlc-ai-nightly-cu116, fastapi, mlc-chat-nightly-cu116\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.7.1\n",
      "    Uninstalling typing_extensions-4.7.1:\n",
      "      Successfully uninstalled typing_extensions-4.7.1\n",
      "  Attempting uninstall: tornado\n",
      "    Found existing installation: tornado 6.3.2\n",
      "    Uninstalling tornado-6.3.2:\n",
      "      Successfully uninstalled tornado-6.3.2\n",
      "  Attempting uninstall: sniffio\n",
      "    Found existing installation: sniffio 1.3.0\n",
      "    Uninstalling sniffio-1.3.0:\n",
      "      Successfully uninstalled sniffio-1.3.0\n",
      "  Attempting uninstall: shortuuid\n",
      "    Found existing installation: shortuuid 1.0.11\n",
      "    Uninstalling shortuuid-1.0.11:\n",
      "      Successfully uninstalled shortuuid-1.0.11\n",
      "  Attempting uninstall: psutil\n",
      "    Found existing installation: psutil 5.9.5\n",
      "    Uninstalling psutil-5.9.5:\n",
      "      Successfully uninstalled psutil-5.9.5\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.25.1\n",
      "    Uninstalling numpy-1.25.1:\n",
      "      Successfully uninstalled numpy-1.25.1\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.4\n",
      "    Uninstalling idna-3.4:\n",
      "      Successfully uninstalled idna-3.4\n",
      "  Attempting uninstall: h11\n",
      "    Found existing installation: h11 0.14.0\n",
      "    Uninstalling h11-0.14.0:\n",
      "      Successfully uninstalled h11-0.14.0\n",
      "  Attempting uninstall: exceptiongroup\n",
      "    Found existing installation: exceptiongroup 1.1.2\n",
      "    Uninstalling exceptiongroup-1.1.2:\n",
      "      Successfully uninstalled exceptiongroup-1.1.2\n",
      "  Attempting uninstall: decorator\n",
      "    Found existing installation: decorator 5.1.1\n",
      "    Uninstalling decorator-5.1.1:\n",
      "      Successfully uninstalled decorator-5.1.1\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 2.2.1\n",
      "    Uninstalling cloudpickle-2.2.1:\n",
      "      Successfully uninstalled cloudpickle-2.2.1\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.1.5\n",
      "    Uninstalling click-8.1.5:\n",
      "      Successfully uninstalled click-8.1.5\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 23.1.0\n",
      "    Uninstalling attrs-23.1.0:\n",
      "      Successfully uninstalled attrs-23.1.0\n",
      "  Attempting uninstall: annotated-types\n",
      "    Found existing installation: annotated-types 0.5.0\n",
      "    Uninstalling annotated-types-0.5.0:\n",
      "      Successfully uninstalled annotated-types-0.5.0\n",
      "  Attempting uninstall: uvicorn\n",
      "    Found existing installation: uvicorn 0.23.0\n",
      "    Uninstalling uvicorn-0.23.0:\n",
      "      Successfully uninstalled uvicorn-0.23.0\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.11.1\n",
      "    Uninstalling scipy-1.11.1:\n",
      "      Successfully uninstalled scipy-1.11.1\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.3.0\n",
      "    Uninstalling pydantic_core-2.3.0:\n",
      "      Successfully uninstalled pydantic_core-2.3.0\n",
      "  Attempting uninstall: ml-dtypes\n",
      "    Found existing installation: ml-dtypes 0.2.0\n",
      "    Uninstalling ml-dtypes-0.2.0:\n",
      "      Successfully uninstalled ml-dtypes-0.2.0\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 3.7.1\n",
      "    Uninstalling anyio-3.7.1:\n",
      "      Successfully uninstalled anyio-3.7.1\n",
      "  Attempting uninstall: starlette\n",
      "    Found existing installation: starlette 0.27.0\n",
      "    Uninstalling starlette-0.27.0:\n",
      "      Successfully uninstalled starlette-0.27.0\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.0.3\n",
      "    Uninstalling pydantic-2.0.3:\n",
      "      Successfully uninstalled pydantic-2.0.3\n",
      "  Attempting uninstall: mlc-ai-nightly-cu116\n",
      "    Found existing installation: mlc-ai-nightly-cu116 0.12.dev1265\n",
      "    Uninstalling mlc-ai-nightly-cu116-0.12.dev1265:\n",
      "      Successfully uninstalled mlc-ai-nightly-cu116-0.12.dev1265\n",
      "  Attempting uninstall: fastapi\n",
      "    Found existing installation: fastapi 0.100.0\n",
      "    Uninstalling fastapi-0.100.0:\n",
      "      Successfully uninstalled fastapi-0.100.0\n",
      "  Attempting uninstall: mlc-chat-nightly-cu116\n",
      "    Found existing installation: mlc-chat-nightly-cu116 0.1.dev257\n",
      "    Uninstalling mlc-chat-nightly-cu116-0.1.dev257:\n",
      "      Successfully uninstalled mlc-chat-nightly-cu116-0.1.dev257\n",
      "Successfully installed annotated-types-0.5.0 anyio-3.7.1 attrs-23.1.0 click-8.1.5 cloudpickle-2.2.1 decorator-5.1.1 exceptiongroup-1.1.2 fastapi-0.100.0 h11-0.14.0 idna-3.4 ml-dtypes-0.2.0 mlc-ai-nightly-cu116-0.12.dev1265 mlc-chat-nightly-cu116-0.1.dev257 numpy-1.25.1 psutil-5.9.5 pydantic-2.0.3 pydantic-core-2.3.0 scipy-1.11.1 shortuuid-1.0.11 sniffio-1.3.0 starlette-0.27.0 tornado-6.3.2 typing-extensions-4.7.1 uvicorn-0.23.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --pre --force-reinstall mlc-ai-nightly-cu116 mlc-chat-nightly-cu116 -f https://mlc.ai/wheels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can clone the [MLC-LLM project](https://github.com/mlc-ai/mlc-llm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'mlc-llm'...\n",
      "remote: Enumerating objects: 5149, done.\u001b[K\n",
      "remote: Counting objects: 100% (1151/1151), done.\u001b[K\n",
      "remote: Compressing objects: 100% (361/361), done.\u001b[K\n",
      "remote: Total 5149 (delta 907), reused 879 (delta 787), pack-reused 3998\u001b[K\n",
      "Receiving objects: 100% (5149/5149), 19.90 MiB | 23.47 MiB/s, done.\n",
      "Resolving deltas: 100% (3217/3217), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone git@github.com:mlc-ai/mlc-llm.git\n",
    "!cd mlc-llm\n",
    "!git submodule update --init --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's download the model weights for the Vicuna-7B model and the prebuilt model libraries from Github. In order to download the large weights, we'll have to use `git lfs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 22.9.0\n",
      "  latest version: 23.5.2\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Retrieving notices: ...working... done\n",
      "Updated git hooks.\n",
      "Git LFS initialized.\n"
     ]
    }
   ],
   "source": [
    "!conda install git git-lfs\n",
    "!git lfs install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'mlc-llm/dist/prebuilt/lib'...\n",
      "remote: Enumerating objects: 142, done.\u001b[K\n",
      "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
      "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
      "remote: Total 142 (delta 1), reused 4 (delta 1), pack-reused 135\u001b[K\n",
      "Receiving objects: 100% (142/142), 40.07 MiB | 22.36 MiB/s, done.\n",
      "Resolving deltas: 100% (94/94), done.\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p mlc-llm/dist/prebuilt\n",
    "!git clone https://github.com/mlc-ai/binary-mlc-llm-libs.git mlc-llm/dist/prebuilt/lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'mlc-chat-vicuna-v1-7b-q3f16_0'...\n",
      "remote: Enumerating objects: 196, done.\u001b[K\n",
      "remote: Counting objects: 100% (196/196), done.\u001b[K\n",
      "remote: Compressing objects: 100% (188/188), done.\u001b[K\n",
      "remote: Total 196 (delta 7), reused 196 (delta 7), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (196/196), 36.70 KiB | 6.12 MiB/s, done.\n",
      "Resolving deltas: 100% (7/7), done.\n",
      "Filtering content: 100% (131/131), 2.84 GiB | 25.55 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://huggingface.co/mlc-ai/mlc-chat-vicuna-v1-7b-q3f16_0\n",
    "!mv mlc-chat-vicuna-v1-7b-q3f16_0 mlc-llm/dist/prebuilt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Chat\n",
    "\n",
    "Before we can chat with the model, we must first import a few libraries and instantiate a `ChatModule` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlc_chat import ChatModule\n",
    "import tvm\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must invoke the `ChatModule` with the appropriate device type, such as `vulkan`, `cuda`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ChatModule(target=\"vulkan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to load the model weights and prebuilt model library into the `ChatModule`, we have to first call the `reload` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib = tvm.runtime.load_module(\"mlc-llm/dist/prebuilt/lib/vicuna-v1-7b-q3f16_0-vulkan.so\")\n",
    "cm.reload(lib=lib, model_path=\"mlc-llm/dist/prebuilt/mlc-chat-vicuna-v1-7b-q3f16_0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all that's needed to set up the `ChatModule`. You can now chat with the model by inputting any prompt you'd like. Try it out below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "prompt = input(\"Prompt: \")\n",
    "cm.prefill(input=prompt)\n",
    "\n",
    "msg = None\n",
    "while not cm.stopped():\n",
    "    cm.decode()\n",
    "    msg = cm.get_message()\n",
    "    clear_output(wait=True)\n",
    "    print(msg, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the speed of the chat bot, you can print some statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prefill: 115.6 tok/s, decode: 30.9 tok/s'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.runtime_stats_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the `ChatModule` will keep a history of your chat. You can reset the chat history by running the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.reset_chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlc-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
