{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Extensions to More Model Variants\n",
        "\n",
        "In the previous tutorial [Compiling Llama-2 with MLC-LLM in Python](https://github.com/mlc-ai/notebooks/blob/main/mlc-llm/tutorial_compile_llama2_with_mlc_llm.ipynb), we saw how to compile a model variant explicitly supported by MLC-LLM (i.e. listed in the [supported model variants](https://mlc.ai/mlc-llm/docs/prebuilt_models.html#supported-model-architectures)). In order to \"explicitly support\" a model variant, it primarily means defining its own [conversation template](https://github.com/mlc-ai/mlc-llm/blob/main/cpp/conv_templates.cc) (e.g. [Gorilla](https://github.com/mlc-ai/mlc-llm/pull/288), [Guanaco](https://github.com/mlc-ai/mlc-llm/pull/497), [WizardLM](https://github.com/mlc-ai/mlc-llm/pull/489)).\n",
        "\n",
        "In this tutorial, we demonstrate that compiling a model variant not on the list is actually quite simple, as long as the architecture is [supported](https://mlc.ai/mlc-llm/docs/prebuilt_models.html#supported-model-architectures) (e.g. `llama`, `rwkv`, `gpt-neox`, etc.). We follow the steps of:\n",
        "0. Environment setup\n",
        "1. Download the weights and build the model\n",
        "2. Update MLC chat configuration JSON\n",
        "3. Chat with the compiled model\n",
        "4. (Optional) Upload the compiled model weights\n",
        "5. (Optional) Use the pre-built model weights you uploaded\n",
        "\n",
        "If you would like to define a new model architecture, you could follow [this tutorial](https://mlc.ai/mlc-llm/docs/tutorials/customize/define_new_models.html), which would be much more involved."
      ],
      "metadata": {
        "id": "jcrY8u1LfPDw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Click the button below to get started!\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/mlc-ai/notebooks/blob/main/mlc-llm/tutorial_extensions_to_more_model_variants.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ],
      "metadata": {
        "id": "MT0Yj_4aAwkY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 0. Environment setup"
      ],
      "metadata": {
        "id": "hjvnlcZSA5dr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will start from setting up the environment. First, let us create a new Conda environment, in which we will run the rest of the notebook.\n",
        "\n",
        "```bash\n",
        "conda create --name mlc-llm python=3.10\n",
        "conda activate mlc-llm\n",
        "```"
      ],
      "metadata": {
        "id": "Uo6hgmZNSh2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Google Colab**\n",
        "- If you are not running this in a Google Colab notebook, you would not need to create a conda environment.\n",
        "- However, be sure to change your runtime to GPU by going to `Runtime` > `Change runtime type` and setting the Hardware accelerator to be \"GPU\".\n",
        "- Besides, compiling some models **may** require more RAM than the default Colab allocates. You may need to either upgrade Colab to a paid plan (so that `runtime shape` can be set to `High RAM`), or use other environments.\n",
        "  - But we also notice that, sometimes rerunning it several times (just the build portion) would successfully pass without exceeding the default RAM amount."
      ],
      "metadata": {
        "id": "IEnMzln5A7l6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are using CUDA, you can run the following command to confirm that CUDA is set up correctly, and check the driver version number as well as what GPUs are currently available for use."
      ],
      "metadata": {
        "id": "YTY16wj6BRGg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uxag_3SZ1TjP"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's download the MLC-AI and MLC-Chat nightly build packages. If you are running in a Colab environment, then you can just run the following command. Otherwise, go to https://mlc.ai/package/ and replace the command below with the one that is appropriate for your hardware and OS."
      ],
      "metadata": {
        "id": "GyZV0EXPBmxo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Google Colab**: If you are using Colab, you may see the red warnings such as **\"You must restart the runtime in order to use newly installed versions.\"** For our purpose, we can disregard them, the notebook will still run correctly."
      ],
      "metadata": {
        "id": "r_1Px4ESBn77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --pre --force-reinstall mlc-ai-nightly-cu118 mlc-chat-nightly-cu118 -f https://mlc.ai/wheels"
      ],
      "metadata": {
        "id": "2q93UKVOuY-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Google Colab**: Since we ignored the warnings/errors in the previous cell, run the following cell to verify the installation did in fact occur properly."
      ],
      "metadata": {
        "id": "_VxF6pJLBwjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -c \"import tvm; print('tvm installed properly!')\"\n",
        "!python -c \"import mlc_chat; print('mlc_chat installed properly!')\""
      ],
      "metadata": {
        "id": "1ts44JURuwGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we clone the [mlc-llm repository](https://github.com/mlc-ai/mlc-llm).\n",
        "\n",
        "**Google Colab**: Note, this will install into the mlc-llm folder. You can click the folder icon on the left menu bar to see the local file system and verify that the repository was cloned successfully."
      ],
      "metadata": {
        "id": "aUkHIONVByuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --recursive https://github.com/mlc-ai/mlc-llm.git"
      ],
      "metadata": {
        "id": "NjtjGDJtuzs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then install `mlc-llm` as a package, so that we can use its functions outside of this directory."
      ],
      "metadata": {
        "id": "N5T2rn8_CC-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd mlc-llm\n",
        "!pip install -e ."
      ],
      "metadata": {
        "id": "svw_jQBLu1Vf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then create a folder to store the downloaded parameters and compiled models. Typically, we store the compiled models under `dist`, and downloaded (i.e. uncompiled) parameters under `dist/models`. This is also the default directory setup for `mlc-llm`."
      ],
      "metadata": {
        "id": "xwQfFrAsCzBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p dist/models"
      ],
      "metadata": {
        "id": "0qEZ4HZ4vlDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have completed setting up the environments. If you are working in a notebook, you would need to run the `exit()` below to restart the runtime. Otherwise, notebooks cannot find the module right after installing them. Simply run this cell, then run the subsequent cells after the runtime finishes restarting."
      ],
      "metadata": {
        "id": "SbPnxB5sECf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "exit()"
      ],
      "metadata": {
        "id": "Jml2Ir4xEA8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1. Download the weights and build the model"
      ],
      "metadata": {
        "id": "XnGGIuoXEWj-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the main section of the tutorial. In order to build the model using the Python function `build_model()`, we use a dataclass `BuildArgs` to organize the arguments for building the model. There are generally two ways of building the model:\n",
        "1. Specify the `hf_path` in the `BuildArgs`, which allows `build_model()` to first download the parameters from hugging face before compiling it.\n",
        "2. Download the parameters yourself, and specify `model` in the `BuildArgs`, so that `build_model()` can locate the downloaded parameters locally.\n",
        "\n",
        "In this tutorial, we will use the first method.\n",
        "\n",
        "**Note**: However, it is worth to note that many model variants post the **parameter delta** on hugging face rather than the actual parameters. For instance, look at the [instructions for compiling WizardLM](https://github.com/mlc-ai/mlc-llm/pull/489). In cases like WizardLM, we will have to proceed with the second method after reconstructing the parameters from the delta.\n",
        "\n",
        "For more details on the arguments, please see [the docs for the CLI's arguments](https://mlc.ai/mlc-llm/docs/compilation/compile_models.html#compile-command-specification) for now, which is equivalent to `BuildArgs`. We will update documentation for `BuildArgs` soon. (Or you could look at the source code)"
      ],
      "metadata": {
        "id": "CYpuwFB-SfJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As mentioned above, we will use the `hf_path` to specify what model variant we would like to compile. Feel free to enter the huggingface path of the model you are interested in below. But please make sure that it contains the actual parameters not the delta.\n",
        "\n",
        "We will use [GOAT-AI's GOAT-7B-Community](https://huggingface.co/GOAT-AI/GOAT-7B-Community) for this tutorial. Note however, that we give other options in the dropdown menu, and hugging face paths can also be used directly."
      ],
      "metadata": {
        "id": "76RI5WzcQXSU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Note: Rerun from this cell if **Google Colab** crashes)"
      ],
      "metadata": {
        "id": "Km__alfbvT1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd mlc-llm\n",
        "!ls"
      ],
      "metadata": {
        "id": "Qby_59HyJKB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# @title Model Parameters\n",
        "hf_path = 'GOAT-AI/GOAT-7B-Community' # @param [\"georgesung/llama2_7b_chat_uncensored\", \"GOAT-AI/GOAT-7B-Community\"] {allow-input: true}"
      ],
      "metadata": {
        "id": "FxWKcInrwRIH",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We import `mlc_llm` that we installed using `pip -p`. `mlc_chat` and `tvm` are included in the nightly pacakges we installed earlier."
      ],
      "metadata": {
        "id": "nV052AlqRc9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mlc_llm\n",
        "import mlc_chat\n",
        "import tvm"
      ],
      "metadata": {
        "id": "dRoae5N5wHkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then specify the arguments for building the model."
      ],
      "metadata": {
        "id": "g5fBsy9ORizF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "build_args = mlc_llm.BuildArgs(\n",
        "    hf_path=hf_path,\n",
        "    quantization=\"q4f16_1\",\n",
        "    target=\"cuda\"\n",
        ")\n",
        "\n",
        "print(build_args)"
      ],
      "metadata": {
        "id": "UC7TSg1QwN4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`mlc_llm.build_model` is the main entrance here. It takes in a `BuildArgs` to start the entire model compilation workflow.\n",
        "\n",
        "**Google Colab** If you are using Colab, the line below may require more RAM than the default Colab provides. You may need to either upgrade to a paid Colab plan, or run it in other environments. (Or sometimes, when you keep rerunning, (just the build portion), it eventually builds without exceeding the RAM Colab provides)"
      ],
      "metadata": {
        "id": "5k9hRYZERt2y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The cell may take ~15 minutes to finish, mainly because downloading the parameters from hugging face takes a while.**"
      ],
      "metadata": {
        "id": "O4xnAUD9SFUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lib_path, model_path, chat_config_path = mlc_llm.build_model(build_args)"
      ],
      "metadata": {
        "id": "ndVTIa_xw7YT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output of `lib_path, model_path, chat_config_path = mlc_llm.build_model(build_args)` is given as a tuple of three paths.\n",
        "\n",
        "`lib_path` is the path to the specific binary that has been built.\n",
        "\n",
        "`model_path` is the path to the folder containing the compiled model parameters and other model specific configuration needed for other `mlc` modules.\n",
        "\n",
        "`chat_config_path` is the path to the specific `.json` configuration needed to have this model work with `mlc_chat`, which we discuss in the next section."
      ],
      "metadata": {
        "id": "ipNK0N5FyQfs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2. Update MLC chat configuration in Python"
      ],
      "metadata": {
        "id": "wJz5k1xRrPEa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first take a look at the `mlc-chat-config.json` file we generated."
      ],
      "metadata": {
        "id": "7L1Z2MvfTZ9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat dist/GOAT-7B-Community-q4f16_1/params/mlc-chat-config.json"
      ],
      "metadata": {
        "id": "7XbM0xHK9t2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that this file contains several parameters that `mlc_chat` needs when running the chat application with this specific model. For example, the `conv_template` we are using for GOAT-7B is `llama_default`, which is defined in [cpp/conv_templates.cc](https://github.com/mlc-ai/mlc-llm/blob/main/cpp/conv_templates.cc).\n",
        "\n",
        "The current logic is that, whenever we compile a model who does not have its own conversation template defined in `cpp/conv_templates.cc` (which is the case for GOAT-7B, unlike say, WizardLM), we concatenate its `model_category` with `_default`, hence `llama_default`.\n",
        "\n",
        "Note that we have not developed a default template for other model categories. In that case, you might need to modify the `mlc-chat-config.json` manually. Perhaps:\n",
        "- Either point the `\"conv_template\"` to one of the defined conversation templates in `cpp/conv_templates.cc`\n",
        "- Then if needed, customize the options in `mlc-chat-config.json` by following the [tutorial here](https://mlc.ai/mlc-llm/docs/get_started/mlc_chat_config.html#configure-mlc-chat-json)\n"
      ],
      "metadata": {
        "id": "GUOgYKJKUhhJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The process of determining the correct `conv_config` may involve some trial and error. Sometimes, the developer may provide useful information on their website.\n",
        "\n",
        "In the case of GOAT-AI, after referring to [the tutorial](https://mlc.ai/mlc-llm/docs/get_started/mlc_chat_config.html#configure-mlc-chat-json), we will change the `stop_str` and `system` entries, so that the JSON config will end up being:\n",
        "\n",
        "```json\n",
        "{\n",
        "    \"model_lib\": \"GOAT-7B-Community-q4f16_1\",\n",
        "    \"local_id\": \"GOAT-7B-Community-q4f16_1\",\n",
        "    \"conv_template\": \"llama_default\",\n",
        "    \"temperature\": 0.7,\n",
        "    \"repetition_penalty\": 1.0,\n",
        "    \"top_p\": 0.95,\n",
        "    \"mean_gen_len\": 128,\n",
        "    \"max_gen_len\": 512,\n",
        "    \"shift_fill_factor\": 0.3,\n",
        "    \"tokenizer_files\": [],\n",
        "    \"model_category\": \"llama\",\n",
        "    \"model_name\": \"GOAT-7B-Community\",\n",
        "    \"conv_config\": {\n",
        "        \"stop_str\": \"\\n\\n\",\n",
        "        \"system\": \"\"\n",
        "    }\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "NYkGIu6McxhY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the `mlc_chat.ConvConfig` and `mlc_chat.ChatConfig` objects to help us with this in Python (so we do not have to manually change the JSON file and can instead programmatically change what we need at runtime). The following is an example that upon calling `chat_module.reset_chat(chat_config)` will give us the desired chat configuraion."
      ],
      "metadata": {
        "id": "BVVPtD4q-4pE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv_config = mlc_chat.ConvConfig(\n",
        "    stop_str='\\n\\n',\n",
        "    system=''\n",
        ")\n",
        "chat_config = mlc_chat.ChatConfig(\n",
        "    conv_template=\"llama_default\",\n",
        "    conv_config=conv_config\n",
        ")"
      ],
      "metadata": {
        "id": "jnbOgNeV9A8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Optional) Step 2b. Update MLC chat configuration JSON"
      ],
      "metadata": {
        "id": "aJxVto2kSW0U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is also possible to modify the JSON directly rather than use the Python API to make these modifications. Simply copy and paste the above modified JSON into the appropriate JSON file to have the changes take effect."
      ],
      "metadata": {
        "id": "75sZX7nFALyC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Google Colab**: To modify the file in google colab, simply click the folder icon on the left, locate the file, and clicking on the file will open up an editor on the right."
      ],
      "metadata": {
        "id": "1dXMfdfcdKrL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3. Chat with the compiled model"
      ],
      "metadata": {
        "id": "AuR540_Rg6VZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can chat using `mlc_chat`'s `ChatModule`. Note that `mlc_llm.build_model` returns the path to the generated files, and we can directly pass them in to the workflow below.\n",
        "\n",
        "For more details on `ChatModule`, please see the other tutorial [Getting Started with MLC-LLM](https://github.com/mlc-ai/notebooks/blob/main/mlc-llm/tutorial_chat_module_getting_started.ipynb), or its documentation [here](https://mlc.ai/mlc-llm/docs/deploy/python.html#api-reference)."
      ],
      "metadata": {
        "id": "KcUlASTNg95F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directly use the returned paths to launch `ChatModule`\n",
        "chat_mod = mlc_chat.ChatModule(model=model_path)\n",
        "chat_mod.reset_chat(chat_config)"
      ],
      "metadata": {
        "id": "wt-FwfLog5t6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Tell me a joke\"\n",
        "chat_mod.generate(prompt=prompt)"
      ],
      "metadata": {
        "id": "sjsOuhhTdlra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Optional) Step 4. Upload the compiled model weights"
      ],
      "metadata": {
        "id": "KqlK8N8SX2v4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we can upload the compiled model weights (the files in `dist/GOAT-7B-Community-q4f16_1/params`) to Hugging Face:\n",
        "\n",
        "```bash\n",
        "# First, please create a repository on Hugging Face.\n",
        "# With the repository created, run\n",
        "git lfs install\n",
        "git clone https://huggingface.co/my-huggingface-account/my-goat7b-weight-huggingface-repo\n",
        "cd my-goat7b-weight-huggingface-repo\n",
        "cp /path/to/dist/GOAT-7B-Community-q4f16_1/params/* .\n",
        "git add . && git commit -m \"Add goat-7b compiled model weights\"\n",
        "git push origin main\n",
        "```\n",
        "\n",
        "We have an example of distributed `GOAT-7B-Community-q4f16_1` on [mlc-ai's huggingface](https://huggingface.co/mlc-ai/mlc-chat-GOAT-7B-Community-q4f16_1/tree/main).\n",
        "\n",
        "The reason why we do not need to upload the `.so` file is because we can reuse the model library, as we will see in the next section."
      ],
      "metadata": {
        "id": "Lmf0EDvQcTWL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Optional) Step 5. Use the pre-built model weights you uploaded"
      ],
      "metadata": {
        "id": "eI_26-pnfb0G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we will show you how to use the model weights you just uploaded. This is similar to what is shown in the tutorial of [Getting Started with Chat Module](https://github.com/mlc-ai/notebooks/blob/main/mlc-llm/tutorial_chat_module_getting_started.ipynb), and we have come full circle."
      ],
      "metadata": {
        "id": "JLyS-O3ziebs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before proceeding, we should restart the runtime again, as for some reason the notebook may crash. Simply run the next cell, and proceed with the following cells after the runtime restarts."
      ],
      "metadata": {
        "id": "9VkFV8fgiu5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "exit()"
      ],
      "metadata": {
        "id": "y-OOvzExit_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To demonstrate the usage of prebuilt weights, we first delete the weights we have downloaded and compiled."
      ],
      "metadata": {
        "id": "FLva6fwen_mA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd mlc-llm && rm -rf dist"
      ],
      "metadata": {
        "id": "Yv-Jcos5id7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we download all the pre-built model libraries (the `.so` file we will use is in here)."
      ],
      "metadata": {
        "id": "-uiny-1ZkdO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p mlc-llm/dist/prebuilt\n",
        "!git clone https://github.com/mlc-ai/binary-mlc-llm-libs.git mlc-llm/dist/prebuilt/lib"
      ],
      "metadata": {
        "id": "CUvB4oqfn04F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, download the pre-built weight you have uploaded to hugging face. Here, we use the example uploaded to mlc-ai's hugging face repo."
      ],
      "metadata": {
        "id": "MvimZX0Lkqm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd mlc-llm/dist/prebuilt && git clone https://huggingface.co/mlc-ai/mlc-chat-GOAT-7B-Community-q4f16_1"
      ],
      "metadata": {
        "id": "1SrUExZgoXSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the model weights we just downloaded from hugging face:"
      ],
      "metadata": {
        "id": "gv25nhL5k3u2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd mlc-llm/dist/prebuilt && ls"
      ],
      "metadata": {
        "id": "f_6o2xU_oG0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is all the pre-built model libraries we cloned.\n",
        "\n",
        "Note that there isn't one for GOAT-7B. However, that is fine because GOAT-7B shares the same architecture with Llama. As long as the model architecture is the same, and the quantization choice is the same, we can reuse the model library! This is why we did not need to upload the `.so` file in Step 4."
      ],
      "metadata": {
        "id": "t4hF9MREk_xT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd mlc-llm/dist/prebuilt/lib && ls"
      ],
      "metadata": {
        "id": "L0Y2CyqAoVRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we follow the same code in Step 3 and chat with the prebuilt model and weights!\n",
        "\n",
        "Notice that the target is now `vulkan`, and the paths now point to the files we just downloaded."
      ],
      "metadata": {
        "id": "QrOQLzIvl2KB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mlc_llm\n",
        "import mlc_chat\n",
        "import tvm"
      ],
      "metadata": {
        "id": "ElqtNXW5ju--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_mod = ChatModule(model=\"mlc-llm/dist/prebuilt/mlc-chat-GOAT-7B-Community-q4f16_1\", lib_path=\"mlc-llm/dist/prebuilt/lib/Llama-2-7b-chat-hf-q4f16_1-cuda.so\", device_name=\"cuda\")"
      ],
      "metadata": {
        "id": "tU2B5Tp4pJaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Write a short poem about Pittsburgh\"\n",
        "chat_mod.generate(prompt=prompt)"
      ],
      "metadata": {
        "id": "tV2kTye9p_Fz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}