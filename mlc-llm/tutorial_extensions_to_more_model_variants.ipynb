{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Extensions to More Model Variants\n",
        "\n",
        "In the previous tutorial [Compiling Llama-2 with MLC-LLM in Python](https://github.com/mlc-ai/notebooks/blob/main/mlc-llm/tutorial_compile_llama2_with_mlc_llm.ipynb), we saw how to compile a model variant explicitly supported by MLC-LLM (i.e. listed in the [supported model variants](https://mlc.ai/mlc-llm/docs/prebuilt_models.html#supported-model-architectures)). In order to \"explicitly support\" a model variant, it primarily means defining its own [conversation template](https://github.com/mlc-ai/mlc-llm/blob/main/cpp/conv_templates.cc) (e.g. [Gorilla](https://github.com/mlc-ai/mlc-llm/pull/288), [Guanaco](https://github.com/mlc-ai/mlc-llm/pull/497), [WizardLM](https://github.com/mlc-ai/mlc-llm/pull/489)).\n",
        "\n",
        "In this tutorial, we demonstrate that compiling a model variant not on the list is actually quite simple, as long as the architecture is [supported](https://mlc.ai/mlc-llm/docs/prebuilt_models.html#supported-model-architectures) (e.g. `llama`, `rwkv`, `gpt-neox`, etc.). We follow the steps of:\n",
        "0. Environment setup\n",
        "1. Download the weights and build the model\n",
        "2. Update MLC chat configuration JSON\n",
        "3. Chat with the compiled model\n",
        "4. (Optional) Upload the compiled model weights\n",
        "5. (Optional) Use the pre-built model weights you uploaded\n",
        "\n",
        "If you would like to define a new model architecture, you could follow [this tutorial](https://mlc.ai/mlc-llm/docs/tutorials/customize/define_new_models.html), which would be much more involved."
      ],
      "metadata": {
        "id": "jcrY8u1LfPDw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Click the button below to get started!\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/mlc-ai/notebooks/blob/main/mlc-llm/tutorial_extensions_to_more_model_variants.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ],
      "metadata": {
        "id": "MT0Yj_4aAwkY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 0. Environment setup"
      ],
      "metadata": {
        "id": "hjvnlcZSA5dr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will start from setting up the environment. First, let us create a new Conda environment, in which we will run the rest of the notebook.\n",
        "\n",
        "```bash\n",
        "conda create --name mlc-llm python=3.10\n",
        "conda activate mlc-llm\n",
        "```"
      ],
      "metadata": {
        "id": "Uo6hgmZNSh2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Google Colab**\n",
        "- If you are running this in a Google Colab notebook, you would not need to create a conda environment.\n",
        "- However, be sure to change your runtime to GPU by going to `Runtime` > `Change runtime type` and setting the Hardware accelerator to be \"GPU\".\n",
        "- Besides, compiling some models **may** require more RAM than the default Colab allocates. You may need to either upgrade Colab to a paid plan (so that `runtime shape` can be set to `High RAM`), or use other environments.\n",
        "  - But we also notice that, sometimes rerunning it several times (just the build portion) would successfully pass without exceeding the default RAM amount."
      ],
      "metadata": {
        "id": "IEnMzln5A7l6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are using CUDA, you can run the following command to confirm that CUDA is set up correctly, and check the driver version number as well as what GPUs are currently available for use."
      ],
      "metadata": {
        "id": "YTY16wj6BRGg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uxag_3SZ1TjP",
        "outputId": "ca46d961-52cc-4a7f-b782-7def93cc70f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Jul 30 07:24:11 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's download the MLC-AI and MLC-Chat nightly build packages. If you are running in a Colab environment, then you can just run the following command. Otherwise, go to https://mlc.ai/package/ and replace the command below with the one that is appropriate for your hardware and OS."
      ],
      "metadata": {
        "id": "GyZV0EXPBmxo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Google Colab**: If you are using Colab, you may see the red warnings such as **\"You must restart the runtime in order to use newly installed versions.\"** For our purpose, we can disregard them, the notebook will still run correctly."
      ],
      "metadata": {
        "id": "r_1Px4ESBn77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --pre --force-reinstall mlc-ai-nightly-cu118 mlc-chat-nightly-cu118 -f https://mlc.ai/wheels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2q93UKVOuY-B",
        "outputId": "d6539cb0-f6f6-43c0-b882-6e043b8a851f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://mlc.ai/wheels\n",
            "Collecting mlc-ai-nightly-cu118\n",
            "  Downloading https://github.com/mlc-ai/package/releases/download/v0.9.dev0/mlc_ai_nightly_cu118-0.12.dev1310-cp310-cp310-manylinux_2_28_x86_64.whl (81.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.2/81.2 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mlc-chat-nightly-cu118\n",
            "  Downloading https://github.com/mlc-ai/package/releases/download/v0.9.dev0/mlc_chat_nightly_cu118-0.1.dev294-cp310-cp310-manylinux_2_28_x86_64.whl (20.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.4/20.4 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting attrs (from mlc-ai-nightly-cu118)\n",
            "  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cloudpickle (from mlc-ai-nightly-cu118)\n",
            "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
            "Collecting decorator (from mlc-ai-nightly-cu118)\n",
            "  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting ml-dtypes (from mlc-ai-nightly-cu118)\n",
            "  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy (from mlc-ai-nightly-cu118)\n",
            "  Downloading numpy-1.25.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting psutil (from mlc-ai-nightly-cu118)\n",
            "  Downloading psutil-5.9.5-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (282 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m282.1/282.1 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy (from mlc-ai-nightly-cu118)\n",
            "  Downloading scipy-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.3/36.3 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tornado (from mlc-ai-nightly-cu118)\n",
            "  Downloading tornado-6.3.2-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (426 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.9/426.9 kB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions (from mlc-ai-nightly-cu118)\n",
            "  Downloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
            "Collecting fastapi (from mlc-chat-nightly-cu118)\n",
            "  Downloading fastapi-0.100.1-py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn (from mlc-chat-nightly-cu118)\n",
            "  Downloading uvicorn-0.23.1-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting shortuuid (from mlc-chat-nightly-cu118)\n",
            "  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
            "Collecting pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 (from fastapi->mlc-chat-nightly-cu118)\n",
            "  Downloading pydantic-2.1.1-py3-none-any.whl (370 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m370.9/370.9 kB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starlette<0.28.0,>=0.27.0 (from fastapi->mlc-chat-nightly-cu118)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting click>=7.0 (from uvicorn->mlc-chat-nightly-cu118)\n",
            "  Downloading click-8.1.6-py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11>=0.8 (from uvicorn->mlc-chat-nightly-cu118)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting annotated-types>=0.4.0 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->fastapi->mlc-chat-nightly-cu118)\n",
            "  Downloading annotated_types-0.5.0-py3-none-any.whl (11 kB)\n",
            "Collecting pydantic-core==2.4.0 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->fastapi->mlc-chat-nightly-cu118)\n",
            "  Downloading pydantic_core-2.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting anyio<5,>=3.4.0 (from starlette<0.28.0,>=0.27.0->fastapi->mlc-chat-nightly-cu118)\n",
            "  Downloading anyio-4.0.0rc1-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.8/82.8 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna>=2.8 (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi->mlc-chat-nightly-cu118)\n",
            "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sniffio>=1.1 (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi->mlc-chat-nightly-cu118)\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Collecting exceptiongroup>=1.0.2 (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi->mlc-chat-nightly-cu118)\n",
            "  Downloading exceptiongroup-1.1.2-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: typing-extensions, tornado, sniffio, shortuuid, psutil, numpy, idna, h11, exceptiongroup, decorator, cloudpickle, click, attrs, annotated-types, uvicorn, scipy, pydantic-core, ml-dtypes, anyio, starlette, pydantic, mlc-ai-nightly-cu118, fastapi, mlc-chat-nightly-cu118\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.7.1\n",
            "    Uninstalling typing_extensions-4.7.1:\n",
            "      Successfully uninstalled typing_extensions-4.7.1\n",
            "  Attempting uninstall: tornado\n",
            "    Found existing installation: tornado 6.3.1\n",
            "    Uninstalling tornado-6.3.1:\n",
            "      Successfully uninstalled tornado-6.3.1\n",
            "  Attempting uninstall: sniffio\n",
            "    Found existing installation: sniffio 1.3.0\n",
            "    Uninstalling sniffio-1.3.0:\n",
            "      Successfully uninstalled sniffio-1.3.0\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.9.5\n",
            "    Uninstalling psutil-5.9.5:\n",
            "      Successfully uninstalled psutil-5.9.5\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.4\n",
            "    Uninstalling numpy-1.22.4:\n",
            "      Successfully uninstalled numpy-1.22.4\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.4\n",
            "    Uninstalling idna-3.4:\n",
            "      Successfully uninstalled idna-3.4\n",
            "  Attempting uninstall: exceptiongroup\n",
            "    Found existing installation: exceptiongroup 1.1.2\n",
            "    Uninstalling exceptiongroup-1.1.2:\n",
            "      Successfully uninstalled exceptiongroup-1.1.2\n",
            "  Attempting uninstall: decorator\n",
            "    Found existing installation: decorator 4.4.2\n",
            "    Uninstalling decorator-4.4.2:\n",
            "      Successfully uninstalled decorator-4.4.2\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 2.2.1\n",
            "    Uninstalling cloudpickle-2.2.1:\n",
            "      Successfully uninstalled cloudpickle-2.2.1\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.1.6\n",
            "    Uninstalling click-8.1.6:\n",
            "      Successfully uninstalled click-8.1.6\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 23.1.0\n",
            "    Uninstalling attrs-23.1.0:\n",
            "      Successfully uninstalled attrs-23.1.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.10.1\n",
            "    Uninstalling scipy-1.10.1:\n",
            "      Successfully uninstalled scipy-1.10.1\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.2.0\n",
            "    Uninstalling ml-dtypes-0.2.0:\n",
            "      Successfully uninstalled ml-dtypes-0.2.0\n",
            "  Attempting uninstall: anyio\n",
            "    Found existing installation: anyio 3.7.1\n",
            "    Uninstalling anyio-3.7.1:\n",
            "      Successfully uninstalled anyio-3.7.1\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.12\n",
            "    Uninstalling pydantic-1.10.12:\n",
            "      Successfully uninstalled pydantic-1.10.12\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "confection 0.1.0 requires pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4, but you have pydantic 2.1.1 which is incompatible.\n",
            "google-colab 1.0.0 requires tornado==6.3.1, but you have tornado 6.3.2 which is incompatible.\n",
            "inflect 6.0.5 requires pydantic<2,>=1.9.1, but you have pydantic 2.1.1 which is incompatible.\n",
            "jupyter-server 1.24.0 requires anyio<4,>=3.1.0, but you have anyio 4.0.0rc1 which is incompatible.\n",
            "moviepy 1.0.3 requires decorator<5.0,>=4.0.2, but you have decorator 5.1.1 which is incompatible.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.25.1 which is incompatible.\n",
            "spacy 3.5.4 requires pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4, but you have pydantic 2.1.1 which is incompatible.\n",
            "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.25.1 which is incompatible.\n",
            "thinc 8.1.10 requires pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4, but you have pydantic 2.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed annotated-types-0.5.0 anyio-4.0.0rc1 attrs-23.1.0 click-8.1.6 cloudpickle-2.2.1 decorator-5.1.1 exceptiongroup-1.1.2 fastapi-0.100.1 h11-0.14.0 idna-3.4 ml-dtypes-0.2.0 mlc-ai-nightly-cu118-0.12.dev1310 mlc-chat-nightly-cu118-0.1.dev294 numpy-1.25.1 psutil-5.9.5 pydantic-2.1.1 pydantic-core-2.4.0 scipy-1.11.1 shortuuid-1.0.11 sniffio-1.3.0 starlette-0.27.0 tornado-6.3.2 typing-extensions-4.7.1 uvicorn-0.23.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "decorator",
                  "numpy",
                  "psutil",
                  "tornado"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Google Colab**: Since we ignored the warnings/errors in the previous cell, run the following cell to verify the installation did in fact occur properly."
      ],
      "metadata": {
        "id": "_VxF6pJLBwjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -c \"import tvm; print('tvm installed properly!')\"\n",
        "!python -c \"import mlc_chat; print('mlc_chat installed properly!')\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ts44JURuwGz",
        "outputId": "2676d244-b8d0-461e-9241-11aa6e725748"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tvm installed properly!\n",
            "mlc_chat installed properly!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we clone the [mlc-llm repository](https://github.com/mlc-ai/mlc-llm).\n",
        "\n",
        "**Google Colab**: Note, this will install into the mlc-llm folder. You can click the folder icon on the left menu bar to see the local file system and verify that the repository was cloned successfully."
      ],
      "metadata": {
        "id": "aUkHIONVByuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --recursive https://github.com/mlc-ai/mlc-llm.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjtjGDJtuzs_",
        "outputId": "506dce98-c4de-4fd4-a65c-b123e27b9493"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mlc-llm'...\n",
            "remote: Enumerating objects: 5489, done.\u001b[K\n",
            "remote: Counting objects: 100% (14/14), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 5489 (delta 0), reused 3 (delta 0), pack-reused 5475\u001b[K\n",
            "Receiving objects: 100% (5489/5489), 20.60 MiB | 34.19 MiB/s, done.\n",
            "Resolving deltas: 100% (3409/3409), done.\n",
            "Submodule '3rdparty/argparse' (https://github.com/p-ranav/argparse) registered for path '3rdparty/argparse'\n",
            "Submodule '3rdparty/googletest' (https://github.com/google/googletest.git) registered for path '3rdparty/googletest'\n",
            "Submodule '3rdparty/tokenizers-cpp' (https://github.com/mlc-ai/tokenizers-cpp) registered for path '3rdparty/tokenizers-cpp'\n",
            "Submodule '3rdparty/tvm' (https://github.com/mlc-ai/relax.git) registered for path '3rdparty/tvm'\n",
            "Cloning into '/content/mlc-llm/3rdparty/argparse'...\n",
            "remote: Enumerating objects: 2421, done.        \n",
            "remote: Counting objects: 100% (27/27), done.        \n",
            "remote: Compressing objects: 100% (10/10), done.        \n",
            "remote: Total 2421 (delta 10), reused 23 (delta 9), pack-reused 2394        \n",
            "Receiving objects: 100% (2421/2421), 740.48 KiB | 11.39 MiB/s, done.\n",
            "Resolving deltas: 100% (1302/1302), done.\n",
            "Cloning into '/content/mlc-llm/3rdparty/googletest'...\n",
            "remote: Enumerating objects: 26670, done.        \n",
            "remote: Counting objects: 100% (244/244), done.        \n",
            "remote: Compressing objects: 100% (139/139), done.        \n",
            "remote: Total 26670 (delta 144), reused 151 (delta 91), pack-reused 26426        \n",
            "Receiving objects: 100% (26670/26670), 12.48 MiB | 26.95 MiB/s, done.\n",
            "Resolving deltas: 100% (19785/19785), done.\n",
            "Cloning into '/content/mlc-llm/3rdparty/tokenizers-cpp'...\n",
            "remote: Enumerating objects: 93, done.        \n",
            "remote: Counting objects: 100% (93/93), done.        \n",
            "remote: Compressing objects: 100% (78/78), done.        \n",
            "remote: Total 93 (delta 28), reused 66 (delta 12), pack-reused 0        \n",
            "Receiving objects: 100% (93/93), 28.79 KiB | 3.20 MiB/s, done.\n",
            "Resolving deltas: 100% (28/28), done.\n",
            "Cloning into '/content/mlc-llm/3rdparty/tvm'...\n",
            "remote: Enumerating objects: 186472, done.        \n",
            "remote: Counting objects: 100% (11145/11145), done.        \n",
            "remote: Compressing objects: 100% (1422/1422), done.        \n",
            "remote: Total 186472 (delta 9831), reused 10684 (delta 9642), pack-reused 175327        \n",
            "Receiving objects: 100% (186472/186472), 69.08 MiB | 29.73 MiB/s, done.\n",
            "Resolving deltas: 100% (145330/145330), done.\n",
            "Submodule path '3rdparty/argparse': checked out '557948f1236db9e27089959de837cc23de6c6bbd'\n",
            "Submodule path '3rdparty/googletest': checked out '45804691223635953f311cf31a10c632553bbfc3'\n",
            "Submodule path '3rdparty/tokenizers-cpp': checked out '6e3a37e2ce4165fb70635a684ad300034fcb63dc'\n",
            "Submodule 'sentencepiece' (https://github.com/google/sentencepiece) registered for path '3rdparty/tokenizers-cpp/sentencepiece'\n",
            "Cloning into '/content/mlc-llm/3rdparty/tokenizers-cpp/sentencepiece'...\n",
            "remote: Enumerating objects: 4815, done.        \n",
            "remote: Counting objects: 100% (1442/1442), done.        \n",
            "remote: Compressing objects: 100% (318/318), done.        \n",
            "remote: Total 4815 (delta 1170), reused 1188 (delta 1084), pack-reused 3373        \n",
            "Receiving objects: 100% (4815/4815), 26.73 MiB | 27.35 MiB/s, done.\n",
            "Resolving deltas: 100% (3309/3309), done.\n",
            "Submodule path '3rdparty/tokenizers-cpp/sentencepiece': checked out 'f2219b53e24ff5deee4cacdc2d0ca3074e529a07'\n",
            "Submodule path '3rdparty/tvm': checked out 'befac6c079a80bf66f68aa7a8b661b1d01671430'\n",
            "Submodule '3rdparty/OpenCL-Headers' (https://github.com/KhronosGroup/OpenCL-Headers.git) registered for path '3rdparty/tvm/3rdparty/OpenCL-Headers'\n",
            "Submodule '3rdparty/cnpy' (https://github.com/rogersce/cnpy.git) registered for path '3rdparty/tvm/3rdparty/cnpy'\n",
            "Submodule '3rdparty/cutlass' (https://github.com/NVIDIA/cutlass.git) registered for path '3rdparty/tvm/3rdparty/cutlass'\n",
            "Submodule '3rdparty/cutlass_fpA_intB_gemm' (https://github.com/tlc-pack/cutlass_fpA_intB_gemm) registered for path '3rdparty/tvm/3rdparty/cutlass_fpA_intB_gemm'\n",
            "Submodule 'dlpack' (https://github.com/dmlc/dlpack.git) registered for path '3rdparty/tvm/3rdparty/dlpack'\n",
            "Submodule 'dmlc-core' (https://github.com/dmlc/dmlc-core.git) registered for path '3rdparty/tvm/3rdparty/dmlc-core'\n",
            "Submodule '3rdparty/libbacktrace' (https://github.com/tlc-pack/libbacktrace.git) registered for path '3rdparty/tvm/3rdparty/libbacktrace'\n",
            "Submodule '3rdparty/rang' (https://github.com/agauniyal/rang.git) registered for path '3rdparty/tvm/3rdparty/rang'\n",
            "Submodule '3rdparty/vta-hw' (https://github.com/apache/tvm-vta.git) registered for path '3rdparty/tvm/3rdparty/vta-hw'\n",
            "Cloning into '/content/mlc-llm/3rdparty/tvm/3rdparty/OpenCL-Headers'...\n",
            "remote: Enumerating objects: 1276, done.        \n",
            "remote: Counting objects: 100% (265/265), done.        \n",
            "remote: Compressing objects: 100% (120/120), done.        \n",
            "remote: Total 1276 (delta 210), reused 164 (delta 142), pack-reused 1011        \n",
            "Receiving objects: 100% (1276/1276), 739.39 KiB | 13.69 MiB/s, done.\n",
            "Resolving deltas: 100% (826/826), done.\n",
            "Cloning into '/content/mlc-llm/3rdparty/tvm/3rdparty/cnpy'...\n",
            "remote: Enumerating objects: 164, done.        \n",
            "remote: Total 164 (delta 0), reused 0 (delta 0), pack-reused 164        \n",
            "Receiving objects: 100% (164/164), 52.32 KiB | 2.09 MiB/s, done.\n",
            "Resolving deltas: 100% (94/94), done.\n",
            "Cloning into '/content/mlc-llm/3rdparty/tvm/3rdparty/cutlass'...\n",
            "remote: Enumerating objects: 21030, done.        \n",
            "remote: Counting objects: 100% (5113/5113), done.        \n",
            "remote: Compressing objects: 100% (449/449), done.        \n",
            "remote: Total 21030 (delta 4777), reused 4688 (delta 4661), pack-reused 15917        \n",
            "Receiving objects: 100% (21030/21030), 30.71 MiB | 29.17 MiB/s, done.\n",
            "Resolving deltas: 100% (15655/15655), done.\n",
            "Cloning into '/content/mlc-llm/3rdparty/tvm/3rdparty/cutlass_fpA_intB_gemm'...\n",
            "remote: Enumerating objects: 292, done.        \n",
            "remote: Counting objects: 100% (292/292), done.        \n",
            "remote: Compressing objects: 100% (122/122), done.        \n",
            "remote: Total 292 (delta 170), reused 270 (delta 151), pack-reused 0        \n",
            "Receiving objects: 100% (292/292), 129.28 KiB | 4.97 MiB/s, done.\n",
            "Resolving deltas: 100% (170/170), done.\n",
            "Cloning into '/content/mlc-llm/3rdparty/tvm/3rdparty/dlpack'...\n",
            "remote: Enumerating objects: 462, done.        \n",
            "remote: Counting objects: 100% (99/99), done.        \n",
            "remote: Compressing objects: 100% (40/40), done.        \n",
            "remote: Total 462 (delta 74), reused 69 (delta 59), pack-reused 363        \n",
            "Receiving objects: 100% (462/462), 1.70 MiB | 18.36 MiB/s, done.\n",
            "Resolving deltas: 100% (162/162), done.\n",
            "Cloning into '/content/mlc-llm/3rdparty/tvm/3rdparty/dmlc-core'...\n",
            "remote: Enumerating objects: 6294, done.        \n",
            "remote: Counting objects: 100% (158/158), done.        \n",
            "remote: Compressing objects: 100% (111/111), done.        \n",
            "remote: Total 6294 (delta 67), reused 98 (delta 30), pack-reused 6136        \n",
            "Receiving objects: 100% (6294/6294), 1.68 MiB | 17.56 MiB/s, done.\n",
            "Resolving deltas: 100% (3813/3813), done.\n",
            "Cloning into '/content/mlc-llm/3rdparty/tvm/3rdparty/libbacktrace'...\n",
            "remote: Enumerating objects: 459, done.        \n",
            "remote: Counting objects: 100% (311/311), done.        \n",
            "remote: Compressing objects: 100% (36/36), done.        \n",
            "remote: Total 459 (delta 285), reused 275 (delta 275), pack-reused 148        \n",
            "Receiving objects: 100% (459/459), 1.03 MiB | 13.74 MiB/s, done.\n",
            "Resolving deltas: 100% (341/341), done.\n",
            "Cloning into '/content/mlc-llm/3rdparty/tvm/3rdparty/rang'...\n",
            "remote: Enumerating objects: 735, done.        \n",
            "remote: Counting objects: 100% (31/31), done.        \n",
            "remote: Compressing objects: 100% (27/27), done.        \n",
            "remote: Total 735 (delta 9), reused 15 (delta 3), pack-reused 704        \n",
            "Receiving objects: 100% (735/735), 265.43 KiB | 6.64 MiB/s, done.\n",
            "Resolving deltas: 100% (371/371), done.\n",
            "Cloning into '/content/mlc-llm/3rdparty/tvm/3rdparty/vta-hw'...\n",
            "remote: Enumerating objects: 3312, done.        \n",
            "remote: Counting objects: 100% (328/328), done.        \n",
            "remote: Compressing objects: 100% (137/137), done.        \n",
            "remote: Total 3312 (delta 256), reused 191 (delta 191), pack-reused 2984        \n",
            "Receiving objects: 100% (3312/3312), 1.43 MiB | 8.27 MiB/s, done.\n",
            "Resolving deltas: 100% (1443/1443), done.\n",
            "Submodule path '3rdparty/tvm/3rdparty/OpenCL-Headers': checked out 'b590a6bfe034ea3a418b7b523e3490956bcb367a'\n",
            "Submodule path '3rdparty/tvm/3rdparty/cnpy': checked out '4e8810b1a8637695171ed346ce68f6984e585ef4'\n",
            "Submodule path '3rdparty/tvm/3rdparty/cutlass': checked out '146d314057c5f193a70c2b36896e739c8c60aef4'\n",
            "Submodule path '3rdparty/tvm/3rdparty/cutlass_fpA_intB_gemm': checked out '390e821fbad2356089aab603d7116c6c820eae65'\n",
            "Submodule 'cutlass' (https://github.com/NVIDIA/cutlass) registered for path '3rdparty/tvm/3rdparty/cutlass_fpA_intB_gemm/cutlass'\n",
            "Cloning into '/content/mlc-llm/3rdparty/tvm/3rdparty/cutlass_fpA_intB_gemm/cutlass'...\n",
            "remote: Enumerating objects: 21030, done.        \n",
            "remote: Counting objects: 100% (5113/5113), done.        \n",
            "remote: Compressing objects: 100% (449/449), done.        \n",
            "remote: Total 21030 (delta 4777), reused 4688 (delta 4661), pack-reused 15917        \n",
            "Receiving objects: 100% (21030/21030), 30.71 MiB | 29.17 MiB/s, done.\n",
            "Resolving deltas: 100% (15655/15655), done.\n",
            "Submodule path '3rdparty/tvm/3rdparty/cutlass_fpA_intB_gemm/cutlass': checked out 'cc85b64cf676c45f98a17e3a47c0aafcf817f088'\n",
            "Submodule path '3rdparty/tvm/3rdparty/dlpack': checked out 'e2bdd3bee8cb6501558042633fa59144cc8b7f5f'\n",
            "Submodule path '3rdparty/tvm/3rdparty/dmlc-core': checked out '09511cf9fe5ff103900a5eafb50870dc84cc17c8'\n",
            "Submodule path '3rdparty/tvm/3rdparty/libbacktrace': checked out '08f7c7e69f8ea61a0c4151359bc8023be8e9217b'\n",
            "Submodule path '3rdparty/tvm/3rdparty/rang': checked out 'cabe04d6d6b05356fa8f9741704924788f0dd762'\n",
            "Submodule path '3rdparty/tvm/3rdparty/vta-hw': checked out '36a91576edf633479c78649e050f18dd2ddc8103'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then install `mlc-llm` as a package, so that we can use its functions outside of this directory."
      ],
      "metadata": {
        "id": "N5T2rn8_CC-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd mlc-llm\n",
        "!pip install -e ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svw_jQBLu1Vf",
        "outputId": "fa7a3a40-cf17-4ad3-f6b0-3b61436f66f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/mlc-llm\n",
            "Obtaining file:///content/mlc-llm\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mlc-llm==0.1.dev295+gd2ccb92) (1.25.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from mlc-llm==0.1.dev295+gd2ccb92) (2.0.1+cu118)\n",
            "Collecting transformers (from mlc-llm==0.1.dev295+gd2ccb92)\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from mlc-llm==0.1.dev295+gd2ccb92) (1.11.1)\n",
            "Collecting timm (from mlc-llm==0.1.dev295+gd2ccb92)\n",
            "  Downloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm->mlc-llm==0.1.dev295+gd2ccb92) (0.15.2+cu118)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm->mlc-llm==0.1.dev295+gd2ccb92) (6.0.1)\n",
            "Collecting huggingface-hub (from timm->mlc-llm==0.1.dev295+gd2ccb92)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors (from timm->mlc-llm==0.1.dev295+gd2ccb92)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->mlc-llm==0.1.dev295+gd2ccb92) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->mlc-llm==0.1.dev295+gd2ccb92) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->mlc-llm==0.1.dev295+gd2ccb92) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->mlc-llm==0.1.dev295+gd2ccb92) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->mlc-llm==0.1.dev295+gd2ccb92) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->mlc-llm==0.1.dev295+gd2ccb92) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->mlc-llm==0.1.dev295+gd2ccb92) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->mlc-llm==0.1.dev295+gd2ccb92) (16.0.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->mlc-llm==0.1.dev295+gd2ccb92) (23.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->mlc-llm==0.1.dev295+gd2ccb92) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->mlc-llm==0.1.dev295+gd2ccb92) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers->mlc-llm==0.1.dev295+gd2ccb92)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m119.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->mlc-llm==0.1.dev295+gd2ccb92) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm->mlc-llm==0.1.dev295+gd2ccb92) (2023.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->mlc-llm==0.1.dev295+gd2ccb92) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mlc-llm==0.1.dev295+gd2ccb92) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mlc-llm==0.1.dev295+gd2ccb92) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mlc-llm==0.1.dev295+gd2ccb92) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mlc-llm==0.1.dev295+gd2ccb92) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->mlc-llm==0.1.dev295+gd2ccb92) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm->mlc-llm==0.1.dev295+gd2ccb92) (9.4.0)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers, timm, mlc-llm\n",
            "  Running setup.py develop for mlc-llm\n",
            "Successfully installed huggingface-hub-0.16.4 mlc-llm-0.1.dev295+gd2ccb92 safetensors-0.3.1 timm-0.9.2 tokenizers-0.13.3 transformers-4.31.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then create a folder to store the downloaded parameters and compiled models. Typically, we store the compiled models under `dist`, and downloaded (i.e. uncompiled) parameters under `dist/models`. This is also the default directory setup for `mlc-llm`."
      ],
      "metadata": {
        "id": "xwQfFrAsCzBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p dist/models"
      ],
      "metadata": {
        "id": "0qEZ4HZ4vlDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have completed setting up the environments. If you are working in a notebook, you would need to run the `exit()` below to restart the runtime. Otherwise, notebooks cannot find the module right after installing them. Simply run this cell, then run the subsequent cells after the runtime finishes restarting."
      ],
      "metadata": {
        "id": "SbPnxB5sECf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "exit()"
      ],
      "metadata": {
        "id": "Jml2Ir4xEA8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1. Download the weights and build the model"
      ],
      "metadata": {
        "id": "XnGGIuoXEWj-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the main section of the tutorial. In order to build the model using the Python function `build_model()`, we use a dataclass `BuildArgs` to organize the arguments for building the model. There are generally two ways of building the model:\n",
        "1. Specify the `hf_path` in the `BuildArgs`, which allows `build_model()` to first download the parameters from hugging face before compiling it.\n",
        "2. Download the parameters yourself, and specify `model` in the `BuildArgs`, so that `build_model()` can locate the downloaded parameters locally.\n",
        "\n",
        "In this tutorial, we will use the first method.\n",
        "\n",
        "**Note**: However, it is worth to note that many model variants post the **parameter delta** on hugging face rather than the actual parameters. For instance, look at the [instructions for compiling WizardLM](https://github.com/mlc-ai/mlc-llm/pull/489). In cases like WizardLM, we will have to proceed with the second method after reconstructing the parameters from the delta.\n",
        "\n",
        "For more details on the arguments, please see [the docs for the CLI's arguments](https://mlc.ai/mlc-llm/docs/compilation/compile_models.html#compile-command-specification) for now, which is equivalent to `BuildArgs`. We will update documentation for `BuildArgs` soon. (Or you could look at the source code)"
      ],
      "metadata": {
        "id": "CYpuwFB-SfJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As mentioned above, we will use the `hf_path` to specify what model variant we would like to compile. Feel free to enter the huggingface path of the model you are interested in below. But please make sure that it contains the actual parameters not the delta.\n",
        "\n",
        "We will use [GOAT-AI's GOAT-7B-Community](https://huggingface.co/GOAT-AI/GOAT-7B-Community) for this tutorial. Note however, that we give other options in the dropdown menu, and hugging face paths can also be used directly."
      ],
      "metadata": {
        "id": "76RI5WzcQXSU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd mlc-llm\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qby_59HyJKB9",
        "outputId": "7b65b0bf-c4b3-46f5-b0a9-d8764c809efe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/mlc-llm\n",
            "3rdparty  CMakeLists.txt   docs      log_db\t       README.md  tests\n",
            "android   CONTRIBUTORS.md  examples  mlc_llm\t       scripts\t  version.py\n",
            "build.py  cpp\t\t   ios\t     mlc_llm.egg-info  setup.py\n",
            "cmake\t  dist\t\t   LICENSE   python\t       site\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# @title Model Parameters\n",
        "hf_path = 'GOAT-AI/GOAT-7B-Community' # @param [\"georgesung/llama2_7b_chat_uncensored\", \"GOAT-AI/GOAT-7B-Community\"] {allow-input: true}"
      ],
      "metadata": {
        "id": "FxWKcInrwRIH",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We import `mlc_llm` that we installed using `pip -p`. `mlc_chat` and `tvm` are included in the nightly pacakges we installed earlier."
      ],
      "metadata": {
        "id": "nV052AlqRc9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mlc_llm\n",
        "import mlc_chat\n",
        "import tvm"
      ],
      "metadata": {
        "id": "dRoae5N5wHkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then specify the arguments for building the model."
      ],
      "metadata": {
        "id": "g5fBsy9ORizF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "build_args = mlc_llm.BuildArgs(\n",
        "    hf_path=hf_path,\n",
        "    quantization=\"q4f16_1\",\n",
        "    target=\"cuda\")\n",
        "\n",
        "print(build_args)"
      ],
      "metadata": {
        "id": "UC7TSg1QwN4l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57eb00b9-e366-46e6-aec4-0a983d25a1f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BuildArgs(model='auto', hf_path='GOAT-AI/GOAT-7B-Community', quantization='q4f16_1', max_seq_len=-1, target='cuda', db_path='log_db', reuse_lib=None, artifact_path='dist', use_cache=1, convert_weight_only=False, build_model_only=False, debug_dump=False, debug_load_script=False, llvm_mingw='', system_lib=False, sep_embed=False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`mlc_llm.build_model` is the main entrance here. It takes in a `BuildArgs` to start the entire model compilation workflow.\n",
        "\n",
        "**Google Colab** If you are using Colab, the line below may require more RAM than the default Colab provides. You may need to either upgrade to a paid Colab plan, or run it in other environments. (Or sometimes, when you keep rerunning, (just the build portion), it eventually builds without exceeding the RAM Colab provides)"
      ],
      "metadata": {
        "id": "5k9hRYZERt2y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The cell may take ~15 minutes to finish, mainly due to downloading the parameters from hugging face takes a while.**"
      ],
      "metadata": {
        "id": "O4xnAUD9SFUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lib_path, model_path, chat_config_path = mlc_llm.build_model(build_args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndVTIa_xw7YT",
        "outputId": "a4034ba5-4837-4b35-e4f8-f215d84d0efa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded weights to dist/models/GOAT-7B-Community\n",
            "Using path \"dist/models/GOAT-7B-Community\" for model \"GOAT-7B-Community\"\n",
            "Database paths: ['log_db/rwkv-raven-7b', 'log_db/redpajama-3b-q4f32', 'log_db/dolly-v2-3b', 'log_db/rwkv-raven-3b', 'log_db/rwkv-raven-1b5', 'log_db/redpajama-3b-q4f16', 'log_db/vicuna-v1-7b']\n",
            "Target configured: cuda -keys=cuda,gpu -arch=sm_75 -max_num_threads=1024 -thread_warp_size=32\n",
            "Automatically using target for weight quantization: cuda -keys=cuda,gpu -arch=sm_75 -max_num_threads=1024 -max_shared_memory_per_block=49152 -max_threads_per_block=1024 -registers_per_block=65536 -thread_warp_size=32\n",
            "Start computing and quantizing weights... This may take a while.\n",
            "Finish computing and quantizing weights.\n",
            "Total param size: 3.5313796997070312 GB\n",
            "Start storing to cache dist/GOAT-7B-Community-q4f16_1/params\n",
            "[0327/0327] saving param_326\n",
            "All finished, 115 total shards committed, record saved to dist/GOAT-7B-Community-q4f16_1/params/ndarray-cache.json\n",
            "Finish exporting chat config to dist/GOAT-7B-Community-q4f16_1/params/mlc-chat-config.json\n",
            "Save a cached module to dist/GOAT-7B-Community-q4f16_1/mod_cache_before_build.pkl.\n",
            "Finish exporting to dist/GOAT-7B-Community-q4f16_1/GOAT-7B-Community-q4f16_1-cuda.so\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output of `lib_path, model_path, chat_config_path = mlc_llm.build_model(build_args)` is given as a tuple of three paths.\n",
        "\n",
        "`lib_path` is the path to the specific binary that has been built.\n",
        "\n",
        "`model_path` is the path to the folder containing the compiled model parameters and other model specific configuration needed for other `mlc` modules.\n",
        "\n",
        "`chat_config_path` is the path to the specific `.json` configuration needed to have this model work with `mlc_chat`."
      ],
      "metadata": {
        "id": "ipNK0N5FyQfs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2. Update MLC chat configuration JSON"
      ],
      "metadata": {
        "id": "aJxVto2kSW0U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We take a look at the `mlc-chat-config.json` file we generated."
      ],
      "metadata": {
        "id": "7L1Z2MvfTZ9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat dist/GOAT-7B-Community-q4f16_1/params/mlc-chat-config.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1bEoMsRNv1Q",
        "outputId": "f0a124e1-1bb0-400d-c9bb-49ca51461743"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"model_lib\": \"GOAT-7B-Community-q4f16_1\",\n",
            "    \"local_id\": \"GOAT-7B-Community-q4f16_1\",\n",
            "    \"conv_template\": \"llama_default\",\n",
            "    \"temperature\": 0.7,\n",
            "    \"repetition_penalty\": 1.0,\n",
            "    \"top_p\": 0.95,\n",
            "    \"mean_gen_len\": 128,\n",
            "    \"max_gen_len\": 512,\n",
            "    \"shift_fill_factor\": 0.3,\n",
            "    \"tokenizer_files\": [],\n",
            "    \"model_category\": \"llama\",\n",
            "    \"model_name\": \"GOAT-7B-Community\"\n",
            "}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that the `conv_template` we are using for GOAT-7B is `llama_default`, which is defined in [cpp/conv_templates.cc](https://github.com/mlc-ai/mlc-llm/blob/main/cpp/conv_templates.cc).\n",
        "\n",
        "The current logic is that, whenever we compile a model who does not have its own conversation template defined in `cpp/conv_templates.cc` (which is the case for GOAT-7B, unlike, say WizardLM), we concatenate its `model_category` with `_default`, hence `llama_default`.\n",
        "\n",
        "Note that we have not developed a default template for other model categories. In that case, you might need to modify the `mlc-chat-config.json` manually. Perhaps:\n",
        "- Either point the `\"conv_template\"` to one of the defined conversation templates in `cpp/conv_templates.cc`\n",
        "- Then if needed, customize the options in `mlc-chat-config.json` by following the [tutorial here](https://mlc.ai/mlc-llm/docs/get_started/mlc_chat_config.html#configure-mlc-chat-json)\n"
      ],
      "metadata": {
        "id": "GUOgYKJKUhhJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The process of determining the correct `conv_config` may involve some trial and error. Sometimes, the developer may provide useful information on their website.\n",
        "\n",
        "In the case of GOAT-AI, after referring to [the tutorial](https://mlc.ai/mlc-llm/docs/get_started/mlc_chat_config.html#configure-mlc-chat-json), we will change the `stop_str` and `system` entries, making the file become:\n",
        "\n",
        "```json\n",
        "{\n",
        "    \"model_lib\": \"GOAT-7B-Community-q4f16_1\",\n",
        "    \"local_id\": \"GOAT-7B-Community-q4f16_1\",\n",
        "    \"conv_template\": \"llama_default\",\n",
        "    \"temperature\": 0.7,\n",
        "    \"repetition_penalty\": 1.0,\n",
        "    \"top_p\": 0.95,\n",
        "    \"mean_gen_len\": 128,\n",
        "    \"max_gen_len\": 512,\n",
        "    \"shift_fill_factor\": 0.3,\n",
        "    \"tokenizer_files\": [],\n",
        "    \"model_category\": \"llama\",\n",
        "    \"model_name\": \"GOAT-7B-Community\",\n",
        "    \"conv_config\": {\n",
        "        \"stop_str\": \"\\n\\n\",\n",
        "        \"system\": \"\"\n",
        "    }\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "NYkGIu6McxhY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Google Colab**: To modify the file in google colab, simply click the folder icon on the left, locate the file, and clicking on the file will open up an editor on the right."
      ],
      "metadata": {
        "id": "1dXMfdfcdKrL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3. Chat with the compiled model"
      ],
      "metadata": {
        "id": "AuR540_Rg6VZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can chat using `mlc_chat`'s `ChatModule`. Note that `mlc_llm.build_model` returns the path to the generated files, and we can directly pass them in to the workflow below.\n",
        "\n",
        "For more details on `ChatModule`, please see the other tutorial [Getting Started with MLC-LLM](https://github.com/mlc-ai/notebooks/blob/main/mlc-llm/tutorial_chat_module_getting_started.ipynb), or its documentation [here](https://mlc.ai/mlc-llm/docs/deploy/python.html#api-reference)."
      ],
      "metadata": {
        "id": "KcUlASTNg95F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directly use the returned paths to launch `ChatModule`\n",
        "lib = tvm.runtime.load_module(lib_path)\n",
        "chat_mod = mlc_chat.ChatModule(target=\"cuda\")\n",
        "chat_mod.reload(lib=lib, model_path=model_path)"
      ],
      "metadata": {
        "id": "wt-FwfLog5t6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "prompt = \"Tell me a joke\"\n",
        "chat_mod.prefill(input=prompt)\n",
        "\n",
        "msg = None\n",
        "while not chat_mod.stopped():\n",
        "    chat_mod.decode()\n",
        "    msg = chat_mod.get_message()\n",
        "    clear_output()\n",
        "    print(msg, flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjsOuhhTdlra",
        "outputId": "740b8de9-60be-485f-bacd-a1eae468e355"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did the tomato turn red? Because it saw the salad dressing!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Optional) Step 4. Upload the compiled model weights"
      ],
      "metadata": {
        "id": "KqlK8N8SX2v4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we can upload the compiled model weights (the files in `dist/GOAT-7B-Community-q4f16_1/params`) to Hugging Face:\n",
        "\n",
        "```bash\n",
        "# First, please create a repository on Hugging Face.\n",
        "# With the repository created, run\n",
        "git lfs install\n",
        "git clone https://huggingface.co/my-huggingface-account/my-goat7b-weight-huggingface-repo\n",
        "cd my-goat7b-weight-huggingface-repo\n",
        "cp /path/to/dist/GOAT-7B-Community-q4f16_1/params/* .\n",
        "git add . && git commit -m \"Add goat-7b compiled model weights\"\n",
        "git push origin main\n",
        "```\n",
        "\n",
        "We have an example of distributed `GOAT-7B-Community-q4f16_1` on [mlc-ai's huggingface](https://huggingface.co/mlc-ai/mlc-chat-GOAT-7B-Community-q4f16_1/tree/main).\n",
        "\n",
        "The reason why we do not need to upload the `.so` file is because we can reuse the model library, as we will see in the next section."
      ],
      "metadata": {
        "id": "Lmf0EDvQcTWL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Optional) Step 5. Use the pre-built model weights you uploaded"
      ],
      "metadata": {
        "id": "eI_26-pnfb0G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we will show you how to use the model weights you just uploaded. This is similar to what is shown in the tutorial of [Getting Started with Chat Module](https://github.com/mlc-ai/notebooks/blob/main/mlc-llm/tutorial_chat_module_getting_started.ipynb), and we have come full circle."
      ],
      "metadata": {
        "id": "JLyS-O3ziebs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before proceeding, we should restart the runtime again, as for some reason the notebook may crash. Simply run the next cell, and proceed with the following cells after the runtime restarts."
      ],
      "metadata": {
        "id": "9VkFV8fgiu5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "exit()"
      ],
      "metadata": {
        "id": "y-OOvzExit_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To demonstrate the usage of prebuilt weights, we first delete the weights we have downloaded and compiled."
      ],
      "metadata": {
        "id": "FLva6fwen_mA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd mlc-llm && rm -rf dist"
      ],
      "metadata": {
        "id": "Yv-Jcos5id7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we download all the pre-built model libraries (the `.so` file we will use is in here)."
      ],
      "metadata": {
        "id": "-uiny-1ZkdO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p mlc-llm/dist/prebuilt\n",
        "!git clone https://github.com/mlc-ai/binary-mlc-llm-libs.git mlc-llm/dist/prebuilt/lib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUvB4oqfn04F",
        "outputId": "cdcf473c-5289-481b-8a18-19ab63006309"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mlc-llm/dist/prebuilt/lib'...\n",
            "remote: Enumerating objects: 191, done.\u001b[K\n",
            "remote: Counting objects: 100% (56/56), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 191 (delta 36), reused 39 (delta 21), pack-reused 135\u001b[K\n",
            "Receiving objects: 100% (191/191), 46.08 MiB | 20.15 MiB/s, done.\n",
            "Resolving deltas: 100% (129/129), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, download the pre-built weight you have uploaded to hugging face. Here, we use the example uploaded to mlc-ai's hugging face repo."
      ],
      "metadata": {
        "id": "MvimZX0Lkqm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd mlc-llm/dist/prebuilt && git clone https://huggingface.co/mlc-ai/mlc-chat-GOAT-7B-Community-q4f16_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SrUExZgoXSn",
        "outputId": "76917347-ee5d-4d76-f5e3-cbcef938058c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mlc-chat-GOAT-7B-Community-q4f16_1'...\n",
            "remote: Enumerating objects: 127, done.\u001b[K\n",
            "remote: Counting objects: 100% (127/127), done.\u001b[K\n",
            "remote: Compressing objects: 100% (126/126), done.\u001b[K\n",
            "remote: Total 127 (delta 2), reused 119 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (127/127), 21.01 KiB | 10.50 MiB/s, done.\n",
            "Resolving deltas: 100% (2/2), done.\n",
            "Filtering content: 100% (116/116), 3.53 GiB | 107.86 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the model weights we just downloaded from hugging face:"
      ],
      "metadata": {
        "id": "gv25nhL5k3u2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd mlc-llm/dist/prebuilt && ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_6o2xU_oG0p",
        "outputId": "9ad244b9-b7d3-4039-a27c-696f9d6f445f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lib  mlc-chat-GOAT-7B-Community-q4f16_1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is all the pre-built model libraries we cloned.\n",
        "\n",
        "Note that there isn't one for GOAT-7B. However, that is fine because GOAT-7B shares the same architecture with Llama. As long as the model architecture is the same, and the quantization choice is the same, we can reuse the model library! This is why we did not need to upload the `.so` file in Step 4."
      ],
      "metadata": {
        "id": "t4hF9MREk_xT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd mlc-llm/dist/prebuilt/lib && ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0Y2CyqAoVRb",
        "outputId": "3d178373-23f0-42ad-b4ed-f2688b9a288a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Llama-2-13b-chat-hf-q4f16_1-metal.so\n",
            "Llama-2-13b-chat-hf-q4f16_1-metal_x86_64.dylib\n",
            "Llama-2-13b-chat-hf-q4f16_1-vulkan.dll\n",
            "Llama-2-13b-chat-hf-q4f16_1-vulkan.so\n",
            "Llama-2-13b-chat-hf-q4f16_1-webgpu.wasm\n",
            "Llama-2-13b-chat-hf-q4f32_1-webgpu.wasm\n",
            "Llama-2-70b-chat-hf-q3f16_1-metal.so\n",
            "Llama-2-70b-chat-hf-q4f16_1-metal.so\n",
            "Llama-2-70b-chat-hf-q4f16_1-webgpu.wasm\n",
            "Llama-2-7b-chat-hf-q3f16_1-iphone.tar\n",
            "Llama-2-7b-chat-hf-q4f16_1-metal.so\n",
            "Llama-2-7b-chat-hf-q4f16_1-metal_x86_64.dylib\n",
            "Llama-2-7b-chat-hf-q4f16_1-vulkan.dll\n",
            "Llama-2-7b-chat-hf-q4f16_1-vulkan.so\n",
            "Llama-2-7b-chat-hf-q4f16_1-webgpu.wasm\n",
            "Llama-2-7b-chat-hf-q4f32_1-webgpu.wasm\n",
            "mlc-chat.apk\n",
            "README.md\n",
            "RedPajama-INCITE-Chat-3B-v1-q4f16_0-iphone.tar\n",
            "RedPajama-INCITE-Chat-3B-v1-q4f16_0-metal.so\n",
            "RedPajama-INCITE-Chat-3B-v1-q4f16_0-metal_x86_64.dylib\n",
            "RedPajama-INCITE-Chat-3B-v1-q4f16_0-vulkan.dll\n",
            "RedPajama-INCITE-Chat-3B-v1-q4f16_0-vulkan.so\n",
            "RedPajama-INCITE-Chat-3B-v1-q4f16_0-webgpu-v1.wasm\n",
            "RedPajama-INCITE-Chat-3B-v1-q4f32_0-webgpu-v1.wasm\n",
            "rwkv-raven-1b5-q8f16_0-metal.so\n",
            "rwkv-raven-1b5-q8f16_0-metal_x86_64.dylib\n",
            "rwkv-raven-1b5-q8f16_0-vulkan.dll\n",
            "rwkv-raven-1b5-q8f16_0-vulkan.so\n",
            "rwkv-raven-3b-q8f16_0-metal.so\n",
            "rwkv-raven-3b-q8f16_0-metal_x86_64.dylib\n",
            "rwkv-raven-3b-q8f16_0-vulkan.dll\n",
            "rwkv-raven-3b-q8f16_0-vulkan.so\n",
            "rwkv-raven-7b-q8f16_0-metal.so\n",
            "rwkv-raven-7b-q8f16_0-metal_x86_64.dylib\n",
            "rwkv-raven-7b-q8f16_0-vulkan.dll\n",
            "rwkv-raven-7b-q8f16_0-vulkan.so\n",
            "tvmjs_runtime_wasi.js\n",
            "vicuna-v1-7b-q3f16_0-iphone.tar\n",
            "vicuna-v1-7b-q3f16_0-metal.so\n",
            "vicuna-v1-7b-q3f16_0-metal_x86_64.dylib\n",
            "vicuna-v1-7b-q3f16_0-vulkan.dll\n",
            "vicuna-v1-7b-q3f16_0-vulkan.so\n",
            "vicuna-v1-7b-q4f32_0-webgpu-v1.wasm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also notice that there is no `cuda` version. Therefore, we will use `Vulkan`."
      ],
      "metadata": {
        "id": "wCHQlsvolfO-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Google Colab**: If you are running this in a Google Colab notebook, you will need to download some Vulkan drivers. You may not need to download the drivers if you are running this locally and already have Vulkan support."
      ],
      "metadata": {
        "id": "BjFQjIYRlocs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install -y vulkan-tools libnvidia-gl-525"
      ],
      "metadata": {
        "id": "BE-TgxRLqTtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following command to confirm that the Vulkan drivers have installed successfully."
      ],
      "metadata": {
        "id": "_GgeHZ4olxNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!vulkaninfo"
      ],
      "metadata": {
        "id": "U8Fwwv5bqYzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we follow the same code in Step 3 and chat with the prebuilt model and weights!\n",
        "\n",
        "Notice that the target is now `vulkan`, and the paths now point to the files we just downloaded."
      ],
      "metadata": {
        "id": "QrOQLzIvl2KB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mlc_llm\n",
        "import mlc_chat\n",
        "import tvm"
      ],
      "metadata": {
        "id": "ElqtNXW5ju--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_mod = mlc_chat.ChatModule(target=\"vulkan\")"
      ],
      "metadata": {
        "id": "tU2B5Tp4pJaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lib = tvm.runtime.load_module(\"mlc-llm/dist/prebuilt/lib/Llama-2-7b-chat-hf-q4f16_1-vulkan.so\")\n",
        "chat_mod.reload(lib=lib, model_path=\"mlc-llm/dist/prebuilt/mlc-chat-GOAT-7B-Community-q4f16_1\")"
      ],
      "metadata": {
        "id": "dHemTOcDpP5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "prompt = \"Prompt: write a short poem about Pittsburgh\"\n",
        "chat_mod.prefill(input=prompt)\n",
        "\n",
        "msg = None\n",
        "while not chat_mod.stopped():\n",
        "    chat_mod.decode()\n",
        "    msg = chat_mod.get_message()\n",
        "    clear_output()\n",
        "    print(msg, flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tV2kTye9p_Fz",
        "outputId": "08eb0637-5fc7-451c-de76-83c78edce696"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pittsburgh, a city of steel,\n",
            "A place where dreams can be realized.\n",
            "From the banks of the Allegheny,\n",
            "To the rolling hills of the Appalachian.\n",
            "A city of bridges, rivers, and parks,\n",
            "Where the world can seem so small and kind.\n",
            "A city of culture, history, and sports,\n",
            "Where dreams can be made in the blink of an eye.\n"
          ]
        }
      ]
    }
  ]
}